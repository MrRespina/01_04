{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc3d3756-fbf7-46f8-b1ef-de98e74a75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3c9b7bf-2761-422d-8093-7285d78b059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_set = np.loadtxt('https://raw.githubusercontent.com/SoongMoo/soldesk20231218/main/data/ThoraricSurgery3.csv',delimiter=',')\n",
    "x = Data_set[:,0:16]\n",
    "y = Data_set[:,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "029bf6ad-be3b-4bed-b984-a0fa936ae61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구조 결정\n",
    "model = Sequential()\n",
    "\n",
    "# 딥러닝 모델의 구조를 결정\n",
    "model.add(Dense(30,input_dim=16,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2cc0243-5868-4975-829e-9bdf2eaad9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 15.6683 - accuracy: 0.1489\n",
      "Epoch 2/5\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 7.3799 - accuracy: 0.1723\n",
      "Epoch 3/5\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.1221 - accuracy: 0.5298\n",
      "Epoch 4/5\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.4923 - accuracy: 0.8511\n",
      "Epoch 5/5\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.4619 - accuracy: 0.8511\n"
     ]
    }
   ],
   "source": [
    "# 모델 실행\n",
    "              #손실함수 / 평균 제곱 오차와 로지스틱 회귀에서 사용한 교차 엔트로피 오차\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# metrics : 모델 수행의 결과를 나타나게끔\n",
    "     # accuracy : 학습셋에 대한 정확도에 기반해 결과를 출력\n",
    "     # loss : 학흡셋에 대한 손실 값을 나타냄\n",
    "     # val_acc : 테스트셋에 대한 정확도를 나타냄\n",
    "     # val_loss : 테스트셋에 대한 손실 값을 나타냄\n",
    "\n",
    "\n",
    "# 딥 러닝 모델을 실행.\n",
    "#                 속성,클래스\n",
    "# batch_size : 데이터를 몇 개의 그룹에 나눠서 실행할 것인지 지정\n",
    "history = model.fit(x,y, epochs=5,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7ca0b4d-cdce-4431-8812-1e7fcfcc4272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[0.09027671]]\n"
     ]
    }
   ],
   "source": [
    "x1 = [6,4.32,3.2,0,0,0,0,0,0,0,0,0,0,0,0,58]\n",
    "prediction = model.predict([x1])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c884ac7-bbd8-4f09-b12d-bca8d667ff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Obtaining dependency information for seaborn from https://files.pythonhosted.org/packages/2d/46/cf3fce41ffc543b6e94dadbe6b647559d591df446ec716e72c3b4ce71b34/seaborn-0.13.1-py3-none-any.whl.metadata\n",
      "  Downloading seaborn-0.13.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\sdedu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn) (1.26.2)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\sdedu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn) (2.1.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\sdedu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn) (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sdedu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sdedu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sdedu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sdedu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sdedu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\sdedu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sdedu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sdedu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sdedu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\sdedu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sdedu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.1-py3-none-any.whl (294 kB)\n",
      "   ---------------------------------------- 0.0/294.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/294.8 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/294.8 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 41.0/294.8 kB 495.5 kB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 122.9/294.8 kB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 294.8/294.8 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30b9afba-d050-4e4f-92e1-30fce2c16797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1dba19e-f437-4cf0-b66a-515236671701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>plasma</th>\n",
       "      <th>pressure</th>\n",
       "      <th>thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnant  plasma  pressure  thickness  insulin   bmi  pedigree  age  \\\n",
       "0         6     148        72         35        0  33.6     0.627   50   \n",
       "1         1      85        66         29        0  26.6     0.351   31   \n",
       "2         8     183        64          0        0  23.3     0.672   32   \n",
       "3         1      89        66         23       94  28.1     0.167   21   \n",
       "4         0     137        40         35      168  43.1     2.288   33   \n",
       "\n",
       "   diabetes  \n",
       "0         1  \n",
       "1         0  \n",
       "2         1  \n",
       "3         0  \n",
       "4         1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 피마 인디언 당뇨병 데이터 셋\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/SoongMoo/soldesk20231218/main/data/pima-indians-diabetes3.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c01d067d-00d0-4b4a-a4e1-4b226dcc1f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>plasma</th>\n",
       "      <th>pressure</th>\n",
       "      <th>thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pregnant      plasma    pressure   thickness     insulin         bmi  \\\n",
       "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
       "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
       "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
       "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
       "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
       "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
       "\n",
       "         pedigree         age    diabetes  \n",
       "count  768.000000  768.000000  768.000000  \n",
       "mean     0.471876   33.240885    0.348958  \n",
       "std      0.331329   11.760232    0.476951  \n",
       "min      0.078000   21.000000    0.000000  \n",
       "25%      0.243750   24.000000    0.000000  \n",
       "50%      0.372500   29.000000    0.000000  \n",
       "75%      0.626250   41.000000    1.000000  \n",
       "max      2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['diabetes'].value_counts()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ad9b12c-3f9a-40be-beca-02785d805983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22d5a2f8d50>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 당뇨인 사람의 공복 혈당과 당뇨가 아닌 사람의 공복 혈당\n",
    "plt.hist(x = [df.plasma[df.diabetes==0],df.plasma[df.diabetes==1]],histtype='barstacked',bins=30,label=['normal','diabetes'])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f3c1fc8-7c8b-44f4-a5bb-022d8c1fef8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22d5a50a6d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(x = [df.bmi[df.diabetes==0],df.bmi[df.diabetes==1]],histtype='barstacked',bins=30,label=['normal','diabetes'])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cfe7d811-58e0-4160-b370-6d1bf10cc298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "154/154 [==============================] - 1s 1ms/step - loss: 2.8034 - accuracy: 0.4870\n",
      "Epoch 2/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.7624 - accuracy: 0.5977\n",
      "Epoch 3/500\n",
      "154/154 [==============================] - 0s 963us/step - loss: 0.7055 - accuracy: 0.6146\n",
      "Epoch 4/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6693 - accuracy: 0.6367\n",
      "Epoch 5/500\n",
      "154/154 [==============================] - 0s 940us/step - loss: 0.6535 - accuracy: 0.6576\n",
      "Epoch 6/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6506 - accuracy: 0.6602\n",
      "Epoch 7/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6425 - accuracy: 0.6680\n",
      "Epoch 8/500\n",
      "154/154 [==============================] - 0s 990us/step - loss: 0.6425 - accuracy: 0.6641\n",
      "Epoch 9/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6323 - accuracy: 0.6680\n",
      "Epoch 10/500\n",
      "154/154 [==============================] - 0s 990us/step - loss: 0.6403 - accuracy: 0.6523\n",
      "Epoch 11/500\n",
      "154/154 [==============================] - 0s 960us/step - loss: 0.6403 - accuracy: 0.6628\n",
      "Epoch 12/500\n",
      "154/154 [==============================] - 0s 945us/step - loss: 0.6376 - accuracy: 0.6549\n",
      "Epoch 13/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6362 - accuracy: 0.6706\n",
      "Epoch 14/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6260 - accuracy: 0.6719\n",
      "Epoch 15/500\n",
      "154/154 [==============================] - 0s 997us/step - loss: 0.6290 - accuracy: 0.6693\n",
      "Epoch 16/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6189 - accuracy: 0.6758\n",
      "Epoch 17/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6235 - accuracy: 0.6836\n",
      "Epoch 18/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6218 - accuracy: 0.6784\n",
      "Epoch 19/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6266 - accuracy: 0.6667\n",
      "Epoch 20/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6150 - accuracy: 0.6836\n",
      "Epoch 21/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6194 - accuracy: 0.6706\n",
      "Epoch 22/500\n",
      "154/154 [==============================] - 0s 950us/step - loss: 0.6178 - accuracy: 0.6732\n",
      "Epoch 23/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6054 - accuracy: 0.6797\n",
      "Epoch 24/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6098 - accuracy: 0.6823\n",
      "Epoch 25/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5965 - accuracy: 0.6849\n",
      "Epoch 26/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5954 - accuracy: 0.6875\n",
      "Epoch 27/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5928 - accuracy: 0.6927\n",
      "Epoch 28/500\n",
      "154/154 [==============================] - 0s 959us/step - loss: 0.5915 - accuracy: 0.7005\n",
      "Epoch 29/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5847 - accuracy: 0.6979\n",
      "Epoch 30/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5811 - accuracy: 0.6992\n",
      "Epoch 31/500\n",
      "154/154 [==============================] - 0s 963us/step - loss: 0.5806 - accuracy: 0.7096\n",
      "Epoch 32/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5766 - accuracy: 0.7044\n",
      "Epoch 33/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5763 - accuracy: 0.7044\n",
      "Epoch 34/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5749 - accuracy: 0.6914\n",
      "Epoch 35/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5742 - accuracy: 0.7005\n",
      "Epoch 36/500\n",
      "154/154 [==============================] - 0s 962us/step - loss: 0.5669 - accuracy: 0.7031\n",
      "Epoch 37/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5710 - accuracy: 0.7122\n",
      "Epoch 38/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5719 - accuracy: 0.7070\n",
      "Epoch 39/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5729 - accuracy: 0.7083\n",
      "Epoch 40/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5713 - accuracy: 0.7161\n",
      "Epoch 41/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5657 - accuracy: 0.6966\n",
      "Epoch 42/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5633 - accuracy: 0.7057\n",
      "Epoch 43/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5575 - accuracy: 0.7148\n",
      "Epoch 44/500\n",
      "154/154 [==============================] - 0s 994us/step - loss: 0.5599 - accuracy: 0.7174\n",
      "Epoch 45/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5578 - accuracy: 0.7122\n",
      "Epoch 46/500\n",
      "154/154 [==============================] - 0s 936us/step - loss: 0.5496 - accuracy: 0.7279\n",
      "Epoch 47/500\n",
      "154/154 [==============================] - 0s 953us/step - loss: 0.5482 - accuracy: 0.7357\n",
      "Epoch 48/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5570 - accuracy: 0.7174\n",
      "Epoch 49/500\n",
      "154/154 [==============================] - 0s 994us/step - loss: 0.5410 - accuracy: 0.7279\n",
      "Epoch 50/500\n",
      "154/154 [==============================] - 0s 986us/step - loss: 0.5420 - accuracy: 0.7344\n",
      "Epoch 51/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5333 - accuracy: 0.7344\n",
      "Epoch 52/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5431 - accuracy: 0.7214\n",
      "Epoch 53/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5404 - accuracy: 0.7305\n",
      "Epoch 54/500\n",
      "154/154 [==============================] - 0s 970us/step - loss: 0.5375 - accuracy: 0.7279\n",
      "Epoch 55/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5337 - accuracy: 0.7383\n",
      "Epoch 56/500\n",
      "154/154 [==============================] - 0s 955us/step - loss: 0.5282 - accuracy: 0.7409\n",
      "Epoch 57/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5344 - accuracy: 0.7344\n",
      "Epoch 58/500\n",
      "154/154 [==============================] - 0s 943us/step - loss: 0.5307 - accuracy: 0.7357\n",
      "Epoch 59/500\n",
      "154/154 [==============================] - 0s 941us/step - loss: 0.5272 - accuracy: 0.7292\n",
      "Epoch 60/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5296 - accuracy: 0.7474\n",
      "Epoch 61/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5195 - accuracy: 0.7396\n",
      "Epoch 62/500\n",
      "154/154 [==============================] - 0s 964us/step - loss: 0.5221 - accuracy: 0.7435\n",
      "Epoch 63/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5321 - accuracy: 0.7331\n",
      "Epoch 64/500\n",
      "154/154 [==============================] - 0s 942us/step - loss: 0.5113 - accuracy: 0.7461\n",
      "Epoch 65/500\n",
      "154/154 [==============================] - 0s 942us/step - loss: 0.5131 - accuracy: 0.7409\n",
      "Epoch 66/500\n",
      "154/154 [==============================] - 0s 944us/step - loss: 0.5289 - accuracy: 0.7383\n",
      "Epoch 67/500\n",
      "154/154 [==============================] - 0s 944us/step - loss: 0.5070 - accuracy: 0.7539\n",
      "Epoch 68/500\n",
      "154/154 [==============================] - 0s 949us/step - loss: 0.5105 - accuracy: 0.7474\n",
      "Epoch 69/500\n",
      "154/154 [==============================] - 0s 944us/step - loss: 0.5082 - accuracy: 0.7435\n",
      "Epoch 70/500\n",
      "154/154 [==============================] - 0s 949us/step - loss: 0.5086 - accuracy: 0.7526\n",
      "Epoch 71/500\n",
      "154/154 [==============================] - 0s 945us/step - loss: 0.5142 - accuracy: 0.7409\n",
      "Epoch 72/500\n",
      "154/154 [==============================] - 0s 942us/step - loss: 0.5190 - accuracy: 0.7474\n",
      "Epoch 73/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5117 - accuracy: 0.7526\n",
      "Epoch 74/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5037 - accuracy: 0.7422\n",
      "Epoch 75/500\n",
      "154/154 [==============================] - 0s 945us/step - loss: 0.5092 - accuracy: 0.7396\n",
      "Epoch 76/500\n",
      "154/154 [==============================] - 0s 952us/step - loss: 0.4945 - accuracy: 0.7591\n",
      "Epoch 77/500\n",
      "154/154 [==============================] - 0s 942us/step - loss: 0.5049 - accuracy: 0.7461\n",
      "Epoch 78/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5092 - accuracy: 0.7461\n",
      "Epoch 79/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5014 - accuracy: 0.7357\n",
      "Epoch 80/500\n",
      "154/154 [==============================] - 0s 952us/step - loss: 0.5099 - accuracy: 0.7500\n",
      "Epoch 81/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5000 - accuracy: 0.7539\n",
      "Epoch 82/500\n",
      "154/154 [==============================] - 0s 940us/step - loss: 0.4926 - accuracy: 0.7630\n",
      "Epoch 83/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4844 - accuracy: 0.7656\n",
      "Epoch 84/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5005 - accuracy: 0.7656\n",
      "Epoch 85/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4962 - accuracy: 0.7669\n",
      "Epoch 86/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4888 - accuracy: 0.7643\n",
      "Epoch 87/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4926 - accuracy: 0.7552\n",
      "Epoch 88/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4876 - accuracy: 0.7539\n",
      "Epoch 89/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4930 - accuracy: 0.7656\n",
      "Epoch 90/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4826 - accuracy: 0.7604\n",
      "Epoch 91/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4830 - accuracy: 0.7669\n",
      "Epoch 92/500\n",
      "154/154 [==============================] - 0s 925us/step - loss: 0.4793 - accuracy: 0.7591\n",
      "Epoch 93/500\n",
      "154/154 [==============================] - 0s 914us/step - loss: 0.4763 - accuracy: 0.7721\n",
      "Epoch 94/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4833 - accuracy: 0.7630\n",
      "Epoch 95/500\n",
      "154/154 [==============================] - 0s 984us/step - loss: 0.4714 - accuracy: 0.7617\n",
      "Epoch 96/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4784 - accuracy: 0.7682\n",
      "Epoch 97/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4751 - accuracy: 0.7773\n",
      "Epoch 98/500\n",
      "154/154 [==============================] - 0s 954us/step - loss: 0.4786 - accuracy: 0.7760\n",
      "Epoch 99/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4880 - accuracy: 0.7578\n",
      "Epoch 100/500\n",
      "154/154 [==============================] - 0s 995us/step - loss: 0.4785 - accuracy: 0.7721\n",
      "Epoch 101/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4725 - accuracy: 0.7643\n",
      "Epoch 102/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4724 - accuracy: 0.7773\n",
      "Epoch 103/500\n",
      "154/154 [==============================] - 0s 985us/step - loss: 0.4747 - accuracy: 0.7734\n",
      "Epoch 104/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4931 - accuracy: 0.7617\n",
      "Epoch 105/500\n",
      "154/154 [==============================] - 0s 998us/step - loss: 0.4782 - accuracy: 0.7826\n",
      "Epoch 106/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4720 - accuracy: 0.7708\n",
      "Epoch 107/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4667 - accuracy: 0.7695\n",
      "Epoch 108/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4879 - accuracy: 0.7656\n",
      "Epoch 109/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4784 - accuracy: 0.7500\n",
      "Epoch 110/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4649 - accuracy: 0.7799\n",
      "Epoch 111/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4722 - accuracy: 0.7682\n",
      "Epoch 112/500\n",
      "154/154 [==============================] - 0s 947us/step - loss: 0.4698 - accuracy: 0.7747\n",
      "Epoch 113/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4710 - accuracy: 0.7786\n",
      "Epoch 114/500\n",
      "154/154 [==============================] - 0s 995us/step - loss: 0.4735 - accuracy: 0.7500\n",
      "Epoch 115/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4785 - accuracy: 0.7565\n",
      "Epoch 116/500\n",
      "154/154 [==============================] - 0s 962us/step - loss: 0.4637 - accuracy: 0.7878\n",
      "Epoch 117/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4595 - accuracy: 0.7682\n",
      "Epoch 118/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4653 - accuracy: 0.7786\n",
      "Epoch 119/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4592 - accuracy: 0.7721\n",
      "Epoch 120/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4598 - accuracy: 0.7708\n",
      "Epoch 121/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4688 - accuracy: 0.7682\n",
      "Epoch 122/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4567 - accuracy: 0.7760\n",
      "Epoch 123/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4654 - accuracy: 0.7812\n",
      "Epoch 124/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4598 - accuracy: 0.7747\n",
      "Epoch 125/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4608 - accuracy: 0.7878\n",
      "Epoch 126/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4548 - accuracy: 0.7812\n",
      "Epoch 127/500\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.4600 - accuracy: 0.7747\n",
      "Epoch 128/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4560 - accuracy: 0.7708\n",
      "Epoch 129/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4573 - accuracy: 0.7721\n",
      "Epoch 130/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4523 - accuracy: 0.7786\n",
      "Epoch 131/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4564 - accuracy: 0.7773\n",
      "Epoch 132/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4598 - accuracy: 0.7786\n",
      "Epoch 133/500\n",
      "154/154 [==============================] - 0s 972us/step - loss: 0.4561 - accuracy: 0.7760\n",
      "Epoch 134/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4616 - accuracy: 0.7773\n",
      "Epoch 135/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4565 - accuracy: 0.7852\n",
      "Epoch 136/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4456 - accuracy: 0.7799\n",
      "Epoch 137/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4432 - accuracy: 0.7995\n",
      "Epoch 138/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4510 - accuracy: 0.7747\n",
      "Epoch 139/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4503 - accuracy: 0.7786\n",
      "Epoch 140/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4599 - accuracy: 0.7734\n",
      "Epoch 141/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4443 - accuracy: 0.7812\n",
      "Epoch 142/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4461 - accuracy: 0.7930\n",
      "Epoch 143/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4479 - accuracy: 0.7852\n",
      "Epoch 144/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4444 - accuracy: 0.7865\n",
      "Epoch 145/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4476 - accuracy: 0.7865\n",
      "Epoch 146/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4521 - accuracy: 0.7943\n",
      "Epoch 147/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4539 - accuracy: 0.7799\n",
      "Epoch 148/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4400 - accuracy: 0.8008\n",
      "Epoch 149/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4564 - accuracy: 0.7852\n",
      "Epoch 150/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4429 - accuracy: 0.7943\n",
      "Epoch 151/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4526 - accuracy: 0.7956\n",
      "Epoch 152/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4400 - accuracy: 0.7995\n",
      "Epoch 153/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4508 - accuracy: 0.7930\n",
      "Epoch 154/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4446 - accuracy: 0.7865\n",
      "Epoch 155/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4356 - accuracy: 0.7956\n",
      "Epoch 156/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4427 - accuracy: 0.7878\n",
      "Epoch 157/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4444 - accuracy: 0.7826\n",
      "Epoch 158/500\n",
      "154/154 [==============================] - 0s 941us/step - loss: 0.4443 - accuracy: 0.7930\n",
      "Epoch 159/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4392 - accuracy: 0.7917\n",
      "Epoch 160/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4310 - accuracy: 0.8021\n",
      "Epoch 161/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4495 - accuracy: 0.7839\n",
      "Epoch 162/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4586 - accuracy: 0.7995\n",
      "Epoch 163/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4458 - accuracy: 0.7865\n",
      "Epoch 164/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4379 - accuracy: 0.8034\n",
      "Epoch 165/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4474 - accuracy: 0.7904\n",
      "Epoch 166/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4382 - accuracy: 0.7865\n",
      "Epoch 167/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4473 - accuracy: 0.7969\n",
      "Epoch 168/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4665 - accuracy: 0.7786\n",
      "Epoch 169/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4370 - accuracy: 0.7969\n",
      "Epoch 170/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4378 - accuracy: 0.7917\n",
      "Epoch 171/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4460 - accuracy: 0.7891\n",
      "Epoch 172/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4395 - accuracy: 0.7865\n",
      "Epoch 173/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4371 - accuracy: 0.7995\n",
      "Epoch 174/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4430 - accuracy: 0.7865\n",
      "Epoch 175/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4295 - accuracy: 0.8112\n",
      "Epoch 176/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4246 - accuracy: 0.8060\n",
      "Epoch 177/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4281 - accuracy: 0.7930\n",
      "Epoch 178/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4350 - accuracy: 0.7852\n",
      "Epoch 179/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4321 - accuracy: 0.7930\n",
      "Epoch 180/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4415 - accuracy: 0.7891\n",
      "Epoch 181/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4379 - accuracy: 0.7904\n",
      "Epoch 182/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4359 - accuracy: 0.8060\n",
      "Epoch 183/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4333 - accuracy: 0.7878\n",
      "Epoch 184/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4328 - accuracy: 0.7995\n",
      "Epoch 185/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4337 - accuracy: 0.7917\n",
      "Epoch 186/500\n",
      "154/154 [==============================] - 0s 981us/step - loss: 0.4370 - accuracy: 0.7956\n",
      "Epoch 187/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4326 - accuracy: 0.8034\n",
      "Epoch 188/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4254 - accuracy: 0.8060\n",
      "Epoch 189/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4366 - accuracy: 0.7956\n",
      "Epoch 190/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4297 - accuracy: 0.7995\n",
      "Epoch 191/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4237 - accuracy: 0.7969\n",
      "Epoch 192/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4244 - accuracy: 0.8138\n",
      "Epoch 193/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4304 - accuracy: 0.7982\n",
      "Epoch 194/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4339 - accuracy: 0.7943\n",
      "Epoch 195/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4293 - accuracy: 0.7891\n",
      "Epoch 196/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4354 - accuracy: 0.7982\n",
      "Epoch 197/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4229 - accuracy: 0.8021\n",
      "Epoch 198/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4204 - accuracy: 0.8086\n",
      "Epoch 199/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4317 - accuracy: 0.7969\n",
      "Epoch 200/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4376 - accuracy: 0.7891\n",
      "Epoch 201/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4259 - accuracy: 0.8034\n",
      "Epoch 202/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4264 - accuracy: 0.8021\n",
      "Epoch 203/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4248 - accuracy: 0.8047\n",
      "Epoch 204/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4230 - accuracy: 0.8151\n",
      "Epoch 205/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4238 - accuracy: 0.8151\n",
      "Epoch 206/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4181 - accuracy: 0.8047\n",
      "Epoch 207/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4212 - accuracy: 0.8034\n",
      "Epoch 208/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4305 - accuracy: 0.7943\n",
      "Epoch 209/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4309 - accuracy: 0.8034\n",
      "Epoch 210/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4246 - accuracy: 0.8086\n",
      "Epoch 211/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4289 - accuracy: 0.7995\n",
      "Epoch 212/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4263 - accuracy: 0.8060\n",
      "Epoch 213/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4222 - accuracy: 0.8086\n",
      "Epoch 214/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4210 - accuracy: 0.8047\n",
      "Epoch 215/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4190 - accuracy: 0.8034\n",
      "Epoch 216/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4203 - accuracy: 0.8164\n",
      "Epoch 217/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4232 - accuracy: 0.8073\n",
      "Epoch 218/500\n",
      "154/154 [==============================] - 0s 936us/step - loss: 0.4189 - accuracy: 0.8151\n",
      "Epoch 219/500\n",
      "154/154 [==============================] - 0s 935us/step - loss: 0.4353 - accuracy: 0.8008\n",
      "Epoch 220/500\n",
      "154/154 [==============================] - 0s 943us/step - loss: 0.4212 - accuracy: 0.8099\n",
      "Epoch 221/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4283 - accuracy: 0.7956\n",
      "Epoch 222/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4185 - accuracy: 0.8190\n",
      "Epoch 223/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4194 - accuracy: 0.8112\n",
      "Epoch 224/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4167 - accuracy: 0.8112\n",
      "Epoch 225/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4234 - accuracy: 0.8086\n",
      "Epoch 226/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4180 - accuracy: 0.8125\n",
      "Epoch 227/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4139 - accuracy: 0.8047\n",
      "Epoch 228/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4185 - accuracy: 0.8073\n",
      "Epoch 229/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4260 - accuracy: 0.8034\n",
      "Epoch 230/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4218 - accuracy: 0.8008\n",
      "Epoch 231/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4255 - accuracy: 0.7956\n",
      "Epoch 232/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4169 - accuracy: 0.8021\n",
      "Epoch 233/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4265 - accuracy: 0.8177\n",
      "Epoch 234/500\n",
      "154/154 [==============================] - 0s 944us/step - loss: 0.4160 - accuracy: 0.8021\n",
      "Epoch 235/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4227 - accuracy: 0.8034\n",
      "Epoch 236/500\n",
      "154/154 [==============================] - 0s 956us/step - loss: 0.4136 - accuracy: 0.8099\n",
      "Epoch 237/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4202 - accuracy: 0.8099\n",
      "Epoch 238/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4101 - accuracy: 0.8073\n",
      "Epoch 239/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4356 - accuracy: 0.7982\n",
      "Epoch 240/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4090 - accuracy: 0.8073\n",
      "Epoch 241/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4182 - accuracy: 0.7969\n",
      "Epoch 242/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4141 - accuracy: 0.8112\n",
      "Epoch 243/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4051 - accuracy: 0.8086\n",
      "Epoch 244/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4166 - accuracy: 0.8112\n",
      "Epoch 245/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4053 - accuracy: 0.8177\n",
      "Epoch 246/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4161 - accuracy: 0.8151\n",
      "Epoch 247/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4065 - accuracy: 0.8138\n",
      "Epoch 248/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4102 - accuracy: 0.8164\n",
      "Epoch 249/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4083 - accuracy: 0.8151\n",
      "Epoch 250/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4148 - accuracy: 0.8099\n",
      "Epoch 251/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4364 - accuracy: 0.7930\n",
      "Epoch 252/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4110 - accuracy: 0.8138\n",
      "Epoch 253/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4162 - accuracy: 0.8255\n",
      "Epoch 254/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4180 - accuracy: 0.7995\n",
      "Epoch 255/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4106 - accuracy: 0.8086\n",
      "Epoch 256/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4166 - accuracy: 0.8203\n",
      "Epoch 257/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4102 - accuracy: 0.8190\n",
      "Epoch 258/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4158 - accuracy: 0.8060\n",
      "Epoch 259/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4087 - accuracy: 0.8112\n",
      "Epoch 260/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4208 - accuracy: 0.8112\n",
      "Epoch 261/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4146 - accuracy: 0.8008\n",
      "Epoch 262/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4172 - accuracy: 0.8164\n",
      "Epoch 263/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4071 - accuracy: 0.8112\n",
      "Epoch 264/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4171 - accuracy: 0.8177\n",
      "Epoch 265/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4130 - accuracy: 0.8229\n",
      "Epoch 266/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4130 - accuracy: 0.8125\n",
      "Epoch 267/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4097 - accuracy: 0.8138\n",
      "Epoch 268/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4281 - accuracy: 0.7995\n",
      "Epoch 269/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4019 - accuracy: 0.8138\n",
      "Epoch 270/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4036 - accuracy: 0.8125\n",
      "Epoch 271/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4157 - accuracy: 0.8099\n",
      "Epoch 272/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4098 - accuracy: 0.8216\n",
      "Epoch 273/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4100 - accuracy: 0.8086\n",
      "Epoch 274/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4015 - accuracy: 0.8333\n",
      "Epoch 275/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4028 - accuracy: 0.8242\n",
      "Epoch 276/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4070 - accuracy: 0.8034\n",
      "Epoch 277/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4055 - accuracy: 0.8164\n",
      "Epoch 278/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4089 - accuracy: 0.8138\n",
      "Epoch 279/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4066 - accuracy: 0.8164\n",
      "Epoch 280/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3998 - accuracy: 0.8177\n",
      "Epoch 281/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3982 - accuracy: 0.8073\n",
      "Epoch 282/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4052 - accuracy: 0.8099\n",
      "Epoch 283/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4013 - accuracy: 0.8255\n",
      "Epoch 284/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4045 - accuracy: 0.8281\n",
      "Epoch 285/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4094 - accuracy: 0.8086\n",
      "Epoch 286/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4034 - accuracy: 0.8125\n",
      "Epoch 287/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4067 - accuracy: 0.8151\n",
      "Epoch 288/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4079 - accuracy: 0.8164\n",
      "Epoch 289/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4110 - accuracy: 0.8203\n",
      "Epoch 290/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4142 - accuracy: 0.8086\n",
      "Epoch 291/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4076 - accuracy: 0.8073\n",
      "Epoch 292/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4096 - accuracy: 0.8138\n",
      "Epoch 293/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4137 - accuracy: 0.7995\n",
      "Epoch 294/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4059 - accuracy: 0.8216\n",
      "Epoch 295/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4000 - accuracy: 0.8112\n",
      "Epoch 296/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4001 - accuracy: 0.8138\n",
      "Epoch 297/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4076 - accuracy: 0.8138\n",
      "Epoch 298/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4036 - accuracy: 0.8294\n",
      "Epoch 299/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4129 - accuracy: 0.8073\n",
      "Epoch 300/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4030 - accuracy: 0.8073\n",
      "Epoch 301/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3960 - accuracy: 0.8112\n",
      "Epoch 302/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4091 - accuracy: 0.8125\n",
      "Epoch 303/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3984 - accuracy: 0.8125\n",
      "Epoch 304/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4013 - accuracy: 0.8229\n",
      "Epoch 305/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4030 - accuracy: 0.8125\n",
      "Epoch 306/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4003 - accuracy: 0.8112\n",
      "Epoch 307/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4022 - accuracy: 0.8138\n",
      "Epoch 308/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4052 - accuracy: 0.8112\n",
      "Epoch 309/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4131 - accuracy: 0.8151\n",
      "Epoch 310/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4042 - accuracy: 0.8151\n",
      "Epoch 311/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4088 - accuracy: 0.8138\n",
      "Epoch 312/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4004 - accuracy: 0.8138\n",
      "Epoch 313/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4113 - accuracy: 0.8138\n",
      "Epoch 314/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3991 - accuracy: 0.8203\n",
      "Epoch 315/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3951 - accuracy: 0.8164\n",
      "Epoch 316/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3954 - accuracy: 0.8190\n",
      "Epoch 317/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3936 - accuracy: 0.8203\n",
      "Epoch 318/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4000 - accuracy: 0.8320\n",
      "Epoch 319/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4018 - accuracy: 0.8242\n",
      "Epoch 320/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3994 - accuracy: 0.8138\n",
      "Epoch 321/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4034 - accuracy: 0.8203\n",
      "Epoch 322/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4086 - accuracy: 0.8073\n",
      "Epoch 323/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4063 - accuracy: 0.8112\n",
      "Epoch 324/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3949 - accuracy: 0.8151\n",
      "Epoch 325/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3969 - accuracy: 0.8138\n",
      "Epoch 326/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3975 - accuracy: 0.8346\n",
      "Epoch 327/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4051 - accuracy: 0.8164\n",
      "Epoch 328/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3911 - accuracy: 0.8164\n",
      "Epoch 329/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3908 - accuracy: 0.8333\n",
      "Epoch 330/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3977 - accuracy: 0.8203\n",
      "Epoch 331/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3871 - accuracy: 0.8164\n",
      "Epoch 332/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3987 - accuracy: 0.8138\n",
      "Epoch 333/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4071 - accuracy: 0.8047\n",
      "Epoch 334/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4052 - accuracy: 0.8177\n",
      "Epoch 335/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3959 - accuracy: 0.8177\n",
      "Epoch 336/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3926 - accuracy: 0.8255\n",
      "Epoch 337/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3984 - accuracy: 0.8216\n",
      "Epoch 338/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3946 - accuracy: 0.8151\n",
      "Epoch 339/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3940 - accuracy: 0.8177\n",
      "Epoch 340/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3901 - accuracy: 0.8294\n",
      "Epoch 341/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4006 - accuracy: 0.8060\n",
      "Epoch 342/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3928 - accuracy: 0.8099\n",
      "Epoch 343/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3922 - accuracy: 0.8203\n",
      "Epoch 344/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3894 - accuracy: 0.8203\n",
      "Epoch 345/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3920 - accuracy: 0.8164\n",
      "Epoch 346/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3978 - accuracy: 0.8112\n",
      "Epoch 347/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3953 - accuracy: 0.8294\n",
      "Epoch 348/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3869 - accuracy: 0.8255\n",
      "Epoch 349/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3974 - accuracy: 0.8151\n",
      "Epoch 350/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4099 - accuracy: 0.8164\n",
      "Epoch 351/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3988 - accuracy: 0.8255\n",
      "Epoch 352/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3886 - accuracy: 0.8216\n",
      "Epoch 353/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3942 - accuracy: 0.8190\n",
      "Epoch 354/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3878 - accuracy: 0.8216\n",
      "Epoch 355/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3883 - accuracy: 0.8177\n",
      "Epoch 356/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4012 - accuracy: 0.8047\n",
      "Epoch 357/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3949 - accuracy: 0.8242\n",
      "Epoch 358/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3915 - accuracy: 0.8216\n",
      "Epoch 359/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3850 - accuracy: 0.8294\n",
      "Epoch 360/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3829 - accuracy: 0.8281\n",
      "Epoch 361/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3843 - accuracy: 0.8281\n",
      "Epoch 362/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3811 - accuracy: 0.8346\n",
      "Epoch 363/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3871 - accuracy: 0.8125\n",
      "Epoch 364/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3988 - accuracy: 0.8255\n",
      "Epoch 365/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3915 - accuracy: 0.8255\n",
      "Epoch 366/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3989 - accuracy: 0.8177\n",
      "Epoch 367/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3896 - accuracy: 0.8203\n",
      "Epoch 368/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3940 - accuracy: 0.8307\n",
      "Epoch 369/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3917 - accuracy: 0.8177\n",
      "Epoch 370/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3800 - accuracy: 0.8268\n",
      "Epoch 371/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4026 - accuracy: 0.8099\n",
      "Epoch 372/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3954 - accuracy: 0.8138\n",
      "Epoch 373/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3944 - accuracy: 0.8138\n",
      "Epoch 374/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3814 - accuracy: 0.8229\n",
      "Epoch 375/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3926 - accuracy: 0.8164\n",
      "Epoch 376/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3924 - accuracy: 0.8177\n",
      "Epoch 377/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3824 - accuracy: 0.8151\n",
      "Epoch 378/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3971 - accuracy: 0.8307\n",
      "Epoch 379/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3977 - accuracy: 0.8203\n",
      "Epoch 380/500\n",
      "154/154 [==============================] - 0s 941us/step - loss: 0.3916 - accuracy: 0.8177\n",
      "Epoch 381/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3922 - accuracy: 0.8216\n",
      "Epoch 382/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3820 - accuracy: 0.8333\n",
      "Epoch 383/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4126 - accuracy: 0.8047\n",
      "Epoch 384/500\n",
      "154/154 [==============================] - 0s 948us/step - loss: 0.3851 - accuracy: 0.8268\n",
      "Epoch 385/500\n",
      "154/154 [==============================] - 0s 989us/step - loss: 0.3904 - accuracy: 0.8268\n",
      "Epoch 386/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3921 - accuracy: 0.8242\n",
      "Epoch 387/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3895 - accuracy: 0.8203\n",
      "Epoch 388/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3804 - accuracy: 0.8281\n",
      "Epoch 389/500\n",
      "154/154 [==============================] - 0s 998us/step - loss: 0.3828 - accuracy: 0.8229\n",
      "Epoch 390/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3911 - accuracy: 0.8242\n",
      "Epoch 391/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4014 - accuracy: 0.8151\n",
      "Epoch 392/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3925 - accuracy: 0.8203\n",
      "Epoch 393/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3846 - accuracy: 0.8229\n",
      "Epoch 394/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3958 - accuracy: 0.8112\n",
      "Epoch 395/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4035 - accuracy: 0.8151\n",
      "Epoch 396/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3786 - accuracy: 0.8242\n",
      "Epoch 397/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4004 - accuracy: 0.8229\n",
      "Epoch 398/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4142 - accuracy: 0.8125\n",
      "Epoch 399/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4059 - accuracy: 0.8138\n",
      "Epoch 400/500\n",
      "154/154 [==============================] - 0s 974us/step - loss: 0.3779 - accuracy: 0.8242\n",
      "Epoch 401/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3910 - accuracy: 0.8099\n",
      "Epoch 402/500\n",
      "154/154 [==============================] - 0s 967us/step - loss: 0.3884 - accuracy: 0.8281\n",
      "Epoch 403/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3855 - accuracy: 0.8138\n",
      "Epoch 404/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3849 - accuracy: 0.8255\n",
      "Epoch 405/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3889 - accuracy: 0.8216\n",
      "Epoch 406/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3856 - accuracy: 0.8294\n",
      "Epoch 407/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3880 - accuracy: 0.8294\n",
      "Epoch 408/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3785 - accuracy: 0.8177\n",
      "Epoch 409/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3860 - accuracy: 0.8255\n",
      "Epoch 410/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4043 - accuracy: 0.8060\n",
      "Epoch 411/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3857 - accuracy: 0.8229\n",
      "Epoch 412/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3918 - accuracy: 0.8216\n",
      "Epoch 413/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3973 - accuracy: 0.8216\n",
      "Epoch 414/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3819 - accuracy: 0.8294\n",
      "Epoch 415/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3845 - accuracy: 0.8216\n",
      "Epoch 416/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3818 - accuracy: 0.8177\n",
      "Epoch 417/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3800 - accuracy: 0.8242\n",
      "Epoch 418/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3820 - accuracy: 0.8242\n",
      "Epoch 419/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3897 - accuracy: 0.8216\n",
      "Epoch 420/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3794 - accuracy: 0.8281\n",
      "Epoch 421/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3953 - accuracy: 0.8320\n",
      "Epoch 422/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3969 - accuracy: 0.8177\n",
      "Epoch 423/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3805 - accuracy: 0.8242\n",
      "Epoch 424/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3950 - accuracy: 0.8177\n",
      "Epoch 425/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3843 - accuracy: 0.8294\n",
      "Epoch 426/500\n",
      "154/154 [==============================] - 0s 988us/step - loss: 0.3746 - accuracy: 0.8255\n",
      "Epoch 427/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3804 - accuracy: 0.8268\n",
      "Epoch 428/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3851 - accuracy: 0.8151\n",
      "Epoch 429/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3883 - accuracy: 0.8138\n",
      "Epoch 430/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3973 - accuracy: 0.8151\n",
      "Epoch 431/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3822 - accuracy: 0.8359\n",
      "Epoch 432/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3799 - accuracy: 0.8255\n",
      "Epoch 433/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3804 - accuracy: 0.8307\n",
      "Epoch 434/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8216\n",
      "Epoch 435/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3822 - accuracy: 0.8216\n",
      "Epoch 436/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3775 - accuracy: 0.8177\n",
      "Epoch 437/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3863 - accuracy: 0.8177\n",
      "Epoch 438/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3764 - accuracy: 0.8307\n",
      "Epoch 439/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3852 - accuracy: 0.8294\n",
      "Epoch 440/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3811 - accuracy: 0.8242\n",
      "Epoch 441/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3897 - accuracy: 0.8255\n",
      "Epoch 442/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3800 - accuracy: 0.8229\n",
      "Epoch 443/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3758 - accuracy: 0.8216\n",
      "Epoch 444/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3792 - accuracy: 0.8268\n",
      "Epoch 445/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3750 - accuracy: 0.8268\n",
      "Epoch 446/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3802 - accuracy: 0.8281\n",
      "Epoch 447/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3842 - accuracy: 0.8164\n",
      "Epoch 448/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3790 - accuracy: 0.8242\n",
      "Epoch 449/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3859 - accuracy: 0.8190\n",
      "Epoch 450/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3893 - accuracy: 0.8177\n",
      "Epoch 451/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3822 - accuracy: 0.8268\n",
      "Epoch 452/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3797 - accuracy: 0.8281\n",
      "Epoch 453/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3893 - accuracy: 0.8242\n",
      "Epoch 454/500\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.3812 - accuracy: 0.8320\n",
      "Epoch 455/500\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.3785 - accuracy: 0.8359\n",
      "Epoch 456/500\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.3825 - accuracy: 0.8268\n",
      "Epoch 457/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3758 - accuracy: 0.8320\n",
      "Epoch 458/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3708 - accuracy: 0.8346\n",
      "Epoch 459/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3955 - accuracy: 0.8164\n",
      "Epoch 460/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3806 - accuracy: 0.8164\n",
      "Epoch 461/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3858 - accuracy: 0.8216\n",
      "Epoch 462/500\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.3737 - accuracy: 0.8255\n",
      "Epoch 463/500\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.3868 - accuracy: 0.8216\n",
      "Epoch 464/500\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.3709 - accuracy: 0.8294\n",
      "Epoch 465/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3815 - accuracy: 0.8294\n",
      "Epoch 466/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3766 - accuracy: 0.8255\n",
      "Epoch 467/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3855 - accuracy: 0.8294\n",
      "Epoch 468/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3863 - accuracy: 0.8281\n",
      "Epoch 469/500\n",
      "154/154 [==============================] - 0s 980us/step - loss: 0.3945 - accuracy: 0.8216\n",
      "Epoch 470/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3721 - accuracy: 0.8307\n",
      "Epoch 471/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3865 - accuracy: 0.8138\n",
      "Epoch 472/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3749 - accuracy: 0.8229\n",
      "Epoch 473/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3771 - accuracy: 0.8216\n",
      "Epoch 474/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3996 - accuracy: 0.8099\n",
      "Epoch 475/500\n",
      "154/154 [==============================] - 0s 959us/step - loss: 0.3656 - accuracy: 0.8164\n",
      "Epoch 476/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3772 - accuracy: 0.8333\n",
      "Epoch 477/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3740 - accuracy: 0.8372\n",
      "Epoch 478/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3883 - accuracy: 0.8268\n",
      "Epoch 479/500\n",
      "154/154 [==============================] - 0s 988us/step - loss: 0.3762 - accuracy: 0.8320\n",
      "Epoch 480/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3912 - accuracy: 0.8125\n",
      "Epoch 481/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3705 - accuracy: 0.8346\n",
      "Epoch 482/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3806 - accuracy: 0.8203\n",
      "Epoch 483/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3799 - accuracy: 0.8346\n",
      "Epoch 484/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3674 - accuracy: 0.8398\n",
      "Epoch 485/500\n",
      "154/154 [==============================] - 0s 974us/step - loss: 0.3893 - accuracy: 0.8216\n",
      "Epoch 486/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3779 - accuracy: 0.8359\n",
      "Epoch 487/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3633 - accuracy: 0.8438\n",
      "Epoch 488/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3741 - accuracy: 0.8294\n",
      "Epoch 489/500\n",
      "154/154 [==============================] - 0s 939us/step - loss: 0.4065 - accuracy: 0.8151\n",
      "Epoch 490/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3760 - accuracy: 0.8281\n",
      "Epoch 491/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.8242\n",
      "Epoch 492/500\n",
      "154/154 [==============================] - 0s 986us/step - loss: 0.3852 - accuracy: 0.8268\n",
      "Epoch 493/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3769 - accuracy: 0.8255\n",
      "Epoch 494/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3785 - accuracy: 0.8164\n",
      "Epoch 495/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3714 - accuracy: 0.8307\n",
      "Epoch 496/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3826 - accuracy: 0.8242\n",
      "Epoch 497/500\n",
      "154/154 [==============================] - 0s 972us/step - loss: 0.3766 - accuracy: 0.8255\n",
      "Epoch 498/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3785 - accuracy: 0.8359\n",
      "Epoch 499/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3767 - accuracy: 0.8333\n",
      "Epoch 500/500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3699 - accuracy: 0.8385\n"
     ]
    }
   ],
   "source": [
    "# 세부 정보를 x로 지정\n",
    "X = df.iloc[:,0:8]\n",
    "# 당뇨 여부를 y로 지정\n",
    "Y = df.iloc[:,8]\n",
    "\n",
    "model = Sequential()  # 모델 객체 생성\n",
    "\n",
    "# 은닉층 생성\n",
    "model.add(Dense(12,input_dim=8,activation='relu',name='Dense_1'))\n",
    "model.add(Dense(8,activation='relu',name='Dense_2'))\n",
    "model.add(Dense(4,activation='relu',name='Dense_3'))\n",
    "model.add(Dense(1,activation='sigmoid',name='Dense_4'))\n",
    "\n",
    "# 모델 실행\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "res = model.fit(X,Y,epochs=500,batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dfe16861-20d1-46cc-b9e7-13443409beee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n",
      "[[0.1056078]]\n"
     ]
    }
   ],
   "source": [
    "x1 = [1,89,66,23,94,28.1,0.167,21]\n",
    "prediction = model.predict([x1])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c87a608f-7de4-4811-b496-928fd29946f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/SoongMoo/soldesk20231218/main/data/iris3.csv')\n",
    "x = df.iloc[:,0:4]\n",
    "y = df.iloc[:,4]\n",
    "\n",
    "# 원-핫 인코딩을 사용해 문자열 처리\n",
    "y = pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c424a66-5aa3-4d11-8e24-205d0b31b3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "30/30 [==============================] - 1s 1ms/step - loss: 1.0589 - accuracy: 0.3933 \n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.0250 - accuracy: 0.3267\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.9972 - accuracy: 0.4867\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.9727 - accuracy: 0.4267\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.9333 - accuracy: 0.6133\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.8941 - accuracy: 0.5800\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.8493 - accuracy: 0.4800\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.8014 - accuracy: 0.6533\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.7588 - accuracy: 0.6667\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.7199 - accuracy: 0.6667\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.6849 - accuracy: 0.6667\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.6511 - accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.6252 - accuracy: 0.6667\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.5976 - accuracy: 0.6667\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.5754 - accuracy: 0.6667\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.5552 - accuracy: 0.6667\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.5390 - accuracy: 0.6667\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.5236 - accuracy: 0.6667\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.5123 - accuracy: 0.6667\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.5011 - accuracy: 0.6667\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.4887 - accuracy: 0.6733\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.4766 - accuracy: 0.6667\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.4737 - accuracy: 0.7867\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.4596 - accuracy: 0.6733\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.4481 - accuracy: 0.7067\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.4394 - accuracy: 0.7667\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.4322 - accuracy: 0.8267\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.4216 - accuracy: 0.7800\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.4150 - accuracy: 0.8000\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.4055 - accuracy: 0.9533\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.7800\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.3910 - accuracy: 0.9533\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.3799 - accuracy: 0.9000\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.3793 - accuracy: 0.8533\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.3616 - accuracy: 0.9667\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.3526 - accuracy: 0.9667\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.3443 - accuracy: 0.9667\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.3365 - accuracy: 0.9467\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.3275 - accuracy: 0.9400\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.9533\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.3075 - accuracy: 0.9533\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.3015 - accuracy: 0.9667\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.2881 - accuracy: 0.9667\n",
      "Epoch 44/100\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2802 - accuracy: 0.9667\n",
      "Epoch 45/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.2732 - accuracy: 0.9667\n",
      "Epoch 46/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.2641 - accuracy: 0.9800\n",
      "Epoch 47/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.2564 - accuracy: 0.9733\n",
      "Epoch 48/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.2493 - accuracy: 0.9667\n",
      "Epoch 49/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.2415 - accuracy: 0.9667\n",
      "Epoch 50/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.2346 - accuracy: 0.9800\n",
      "Epoch 51/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.2230 - accuracy: 0.9733\n",
      "Epoch 52/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.2185 - accuracy: 0.9733\n",
      "Epoch 53/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.2105 - accuracy: 0.9667\n",
      "Epoch 54/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.2037 - accuracy: 0.9733\n",
      "Epoch 55/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.2016 - accuracy: 0.9667\n",
      "Epoch 56/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9600\n",
      "Epoch 57/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1851 - accuracy: 0.9667\n",
      "Epoch 58/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9733\n",
      "Epoch 59/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.9733\n",
      "Epoch 60/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.9667\n",
      "Epoch 61/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9733\n",
      "Epoch 62/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1616 - accuracy: 0.9667\n",
      "Epoch 63/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1620 - accuracy: 0.9800\n",
      "Epoch 64/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9667\n",
      "Epoch 65/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1473 - accuracy: 0.9733\n",
      "Epoch 66/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1471 - accuracy: 0.9733\n",
      "Epoch 67/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1452 - accuracy: 0.9800\n",
      "Epoch 68/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.9800\n",
      "Epoch 69/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1401 - accuracy: 0.9667\n",
      "Epoch 70/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.9800\n",
      "Epoch 71/100\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1308 - accuracy: 0.9733\n",
      "Epoch 72/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.9667\n",
      "Epoch 73/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.9733\n",
      "Epoch 74/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9800\n",
      "Epoch 75/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9733\n",
      "Epoch 76/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9733\n",
      "Epoch 77/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9733\n",
      "Epoch 78/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1161 - accuracy: 0.9733\n",
      "Epoch 79/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9733\n",
      "Epoch 80/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9667\n",
      "Epoch 81/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9733\n",
      "Epoch 82/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9800\n",
      "Epoch 83/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1073 - accuracy: 0.9667\n",
      "Epoch 84/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9733\n",
      "Epoch 85/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1060 - accuracy: 0.9733\n",
      "Epoch 86/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9733\n",
      "Epoch 87/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.9667\n",
      "Epoch 88/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.9667\n",
      "Epoch 89/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9667\n",
      "Epoch 90/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.9733\n",
      "Epoch 91/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.9733\n",
      "Epoch 92/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9600\n",
      "Epoch 93/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9667\n",
      "Epoch 94/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.9667\n",
      "Epoch 95/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.9800\n",
      "Epoch 96/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.9733\n",
      "Epoch 97/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9800\n",
      "Epoch 98/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9667\n",
      "Epoch 99/100\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.9667\n",
      "Epoch 100/100\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1003 - accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x22d5d066b50>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(12,input_dim=4,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(3,activation='softmax'))    # 다중 분류인 경우는 softmax 사용.\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])   # 소프트맥스를 사용하는 경우, 손실 함수는 categorical_crossentropy 사용.\n",
    "\n",
    "model.fit(x,y,epochs=100,batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c227a395-23ff-4453-8dd9-6057dbae1b9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node sequential_27/dense_79/Relu defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1077, in launch_instance\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Temp\\ipykernel_2032\\425592861.py\", line 2, in <module>\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2655, in predict\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 590, in __call__\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\sequential.py\", line 398, in call\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 515, in call\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 672, in _run_internal_graph\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py\", line 255, in call\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\activations.py\", line 306, in relu\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py\", line 5395, in relu\n\nMatrix size-incompatible: In[0]: [1,4], In[1]: [60,24]\n\t [[{{node sequential_27/dense_79/Relu}}]] [Op:__inference_predict_function_636569]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x11 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m2.2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx11\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predict)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(predict)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sequential_27/dense_79/Relu defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1077, in launch_instance\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Temp\\ipykernel_2032\\425592861.py\", line 2, in <module>\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2655, in predict\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 590, in __call__\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\sequential.py\", line 398, in call\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 515, in call\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 672, in _run_internal_graph\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py\", line 255, in call\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\activations.py\", line 306, in relu\n\n  File \"C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py\", line 5395, in relu\n\nMatrix size-incompatible: In[0]: [1,4], In[1]: [60,24]\n\t [[{{node sequential_27/dense_79/Relu}}]] [Op:__inference_predict_function_636569]"
     ]
    }
   ],
   "source": [
    "x11 = [6,2.2,4,1]\n",
    "predict = model.predict([x11])\n",
    "predicted_class = np.argmax(predict)\n",
    "\n",
    "print(predict)\n",
    "print(f'predicted_class = {predicted_class}')\n",
    "\n",
    "if predicted_class == 0:\n",
    "    print('iris-setosa')\n",
    "elif predicted_class == 1:\n",
    "    print('iris-versicolor')\n",
    "elif predicted_class == 2:\n",
    "    print('iris-verginica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4151c2-1d04-4fea-b80f-84febbc9b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python.exe -m pip install --upgrade pip\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6aefec4d-e27c-4409-ba4f-83741222c318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "29/29 [==============================] - 1s 1ms/step - loss: 0.6866 - accuracy: 0.5139\n",
      "Epoch 2/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.6775 - accuracy: 0.5278\n",
      "Epoch 3/200\n",
      "29/29 [==============================] - 0s 986us/step - loss: 0.6691 - accuracy: 0.5486\n",
      "Epoch 4/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.6607 - accuracy: 0.5625\n",
      "Epoch 5/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.6469 - accuracy: 0.5972\n",
      "Epoch 6/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.6386 - accuracy: 0.6042\n",
      "Epoch 7/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.6262 - accuracy: 0.6458\n",
      "Epoch 8/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.6058 - accuracy: 0.6806\n",
      "Epoch 9/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5926 - accuracy: 0.7361\n",
      "Epoch 10/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5800 - accuracy: 0.7431\n",
      "Epoch 11/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5769 - accuracy: 0.7361\n",
      "Epoch 12/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5707 - accuracy: 0.7639\n",
      "Epoch 13/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5506 - accuracy: 0.7778\n",
      "Epoch 14/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5458 - accuracy: 0.7986\n",
      "Epoch 15/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5353 - accuracy: 0.7917\n",
      "Epoch 16/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5250 - accuracy: 0.7917\n",
      "Epoch 17/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5235 - accuracy: 0.7708\n",
      "Epoch 18/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5113 - accuracy: 0.8125\n",
      "Epoch 19/200\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5135 - accuracy: 0.8056\n",
      "Epoch 20/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5044 - accuracy: 0.8264\n",
      "Epoch 21/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5031 - accuracy: 0.8056\n",
      "Epoch 22/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4933 - accuracy: 0.8125\n",
      "Epoch 23/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4946 - accuracy: 0.8264\n",
      "Epoch 24/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5191 - accuracy: 0.8056\n",
      "Epoch 25/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4795 - accuracy: 0.8333\n",
      "Epoch 26/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4788 - accuracy: 0.8264\n",
      "Epoch 27/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4667 - accuracy: 0.8264\n",
      "Epoch 28/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4618 - accuracy: 0.8681\n",
      "Epoch 29/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4597 - accuracy: 0.8472\n",
      "Epoch 30/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4465 - accuracy: 0.8542\n",
      "Epoch 31/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4566 - accuracy: 0.8125\n",
      "Epoch 32/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4414 - accuracy: 0.8819\n",
      "Epoch 33/200\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4461 - accuracy: 0.8681\n",
      "Epoch 34/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4485 - accuracy: 0.8125\n",
      "Epoch 35/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4406 - accuracy: 0.8611\n",
      "Epoch 36/200\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4545 - accuracy: 0.7917\n",
      "Epoch 37/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4454 - accuracy: 0.8403\n",
      "Epoch 38/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4284 - accuracy: 0.8819\n",
      "Epoch 39/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4179 - accuracy: 0.8681\n",
      "Epoch 40/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4092 - accuracy: 0.8750\n",
      "Epoch 41/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4019 - accuracy: 0.8750\n",
      "Epoch 42/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4008 - accuracy: 0.8681\n",
      "Epoch 43/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3898 - accuracy: 0.8819\n",
      "Epoch 44/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3833 - accuracy: 0.8819\n",
      "Epoch 45/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3811 - accuracy: 0.8889\n",
      "Epoch 46/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3898 - accuracy: 0.8750\n",
      "Epoch 47/200\n",
      "29/29 [==============================] - 0s 901us/step - loss: 0.3719 - accuracy: 0.8889\n",
      "Epoch 48/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3671 - accuracy: 0.8958\n",
      "Epoch 49/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3643 - accuracy: 0.8958\n",
      "Epoch 50/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3469 - accuracy: 0.9167\n",
      "Epoch 51/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3697 - accuracy: 0.9097\n",
      "Epoch 52/200\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3530 - accuracy: 0.9236\n",
      "Epoch 53/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3368 - accuracy: 0.9236\n",
      "Epoch 54/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3335 - accuracy: 0.9375\n",
      "Epoch 55/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3238 - accuracy: 0.9097\n",
      "Epoch 56/200\n",
      "29/29 [==============================] - 0s 961us/step - loss: 0.3296 - accuracy: 0.8889\n",
      "Epoch 57/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3197 - accuracy: 0.9306\n",
      "Epoch 58/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3655 - accuracy: 0.8750\n",
      "Epoch 59/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3127 - accuracy: 0.9236\n",
      "Epoch 60/200\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3080 - accuracy: 0.9306\n",
      "Epoch 61/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3009 - accuracy: 0.9375\n",
      "Epoch 62/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3043 - accuracy: 0.9236\n",
      "Epoch 63/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3178 - accuracy: 0.8958\n",
      "Epoch 64/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3089 - accuracy: 0.9097\n",
      "Epoch 65/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2950 - accuracy: 0.9236\n",
      "Epoch 66/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2957 - accuracy: 0.9167\n",
      "Epoch 67/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2662 - accuracy: 0.9514\n",
      "Epoch 68/200\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2690 - accuracy: 0.9444\n",
      "Epoch 69/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2594 - accuracy: 0.9306\n",
      "Epoch 70/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2552 - accuracy: 0.9514\n",
      "Epoch 71/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2545 - accuracy: 0.9444\n",
      "Epoch 72/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2437 - accuracy: 0.9444\n",
      "Epoch 73/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2497 - accuracy: 0.9444\n",
      "Epoch 74/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2405 - accuracy: 0.9375\n",
      "Epoch 75/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2341 - accuracy: 0.9653\n",
      "Epoch 76/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2447 - accuracy: 0.9306\n",
      "Epoch 77/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2395 - accuracy: 0.9514\n",
      "Epoch 78/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2480 - accuracy: 0.9375\n",
      "Epoch 79/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2274 - accuracy: 0.9653\n",
      "Epoch 80/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2164 - accuracy: 0.9583\n",
      "Epoch 81/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2396 - accuracy: 0.9306\n",
      "Epoch 82/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2054 - accuracy: 0.9722\n",
      "Epoch 83/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2100 - accuracy: 0.9583\n",
      "Epoch 84/200\n",
      "29/29 [==============================] - 0s 964us/step - loss: 0.2099 - accuracy: 0.9514\n",
      "Epoch 85/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2100 - accuracy: 0.9653\n",
      "Epoch 86/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2078 - accuracy: 0.9514\n",
      "Epoch 87/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2056 - accuracy: 0.9514\n",
      "Epoch 88/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1819 - accuracy: 0.9722\n",
      "Epoch 89/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9861\n",
      "Epoch 90/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1771 - accuracy: 0.9792\n",
      "Epoch 91/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1732 - accuracy: 0.9792\n",
      "Epoch 92/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1747 - accuracy: 0.9792\n",
      "Epoch 93/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1678 - accuracy: 0.9792\n",
      "Epoch 94/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1723 - accuracy: 0.9792\n",
      "Epoch 95/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1628 - accuracy: 0.9861\n",
      "Epoch 96/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1634 - accuracy: 0.9861\n",
      "Epoch 97/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1566 - accuracy: 0.9861\n",
      "Epoch 98/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9861\n",
      "Epoch 99/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.9792\n",
      "Epoch 100/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1568 - accuracy: 0.9722\n",
      "Epoch 101/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2058 - accuracy: 0.9514\n",
      "Epoch 102/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.9722\n",
      "Epoch 103/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1469 - accuracy: 0.9792\n",
      "Epoch 104/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1422 - accuracy: 0.9861\n",
      "Epoch 105/200\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1366 - accuracy: 0.9861\n",
      "Epoch 106/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9792\n",
      "Epoch 107/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1441 - accuracy: 0.9792\n",
      "Epoch 108/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9861\n",
      "Epoch 109/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9861\n",
      "Epoch 110/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1325 - accuracy: 0.9861\n",
      "Epoch 111/200\n",
      "29/29 [==============================] - 0s 773us/step - loss: 0.1387 - accuracy: 0.9861\n",
      "Epoch 112/200\n",
      "29/29 [==============================] - 0s 952us/step - loss: 0.1316 - accuracy: 0.9792\n",
      "Epoch 113/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9722\n",
      "Epoch 114/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9861\n",
      "Epoch 115/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9861\n",
      "Epoch 116/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9861\n",
      "Epoch 117/200\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1228 - accuracy: 0.9861\n",
      "Epoch 118/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9861\n",
      "Epoch 119/200\n",
      "29/29 [==============================] - 0s 951us/step - loss: 0.1142 - accuracy: 0.9861\n",
      "Epoch 120/200\n",
      "29/29 [==============================] - 0s 950us/step - loss: 0.1140 - accuracy: 0.9861\n",
      "Epoch 121/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9861\n",
      "Epoch 122/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9861\n",
      "Epoch 123/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9861\n",
      "Epoch 124/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9861\n",
      "Epoch 125/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9861\n",
      "Epoch 126/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1097 - accuracy: 0.9861\n",
      "Epoch 127/200\n",
      "29/29 [==============================] - 0s 865us/step - loss: 0.1145 - accuracy: 0.9861\n",
      "Epoch 128/200\n",
      "29/29 [==============================] - 0s 863us/step - loss: 0.1287 - accuracy: 0.9792\n",
      "Epoch 129/200\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1098 - accuracy: 0.9861\n",
      "Epoch 130/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9861\n",
      "Epoch 131/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9861\n",
      "Epoch 132/200\n",
      "29/29 [==============================] - 0s 808us/step - loss: 0.1014 - accuracy: 0.9861\n",
      "Epoch 133/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1002 - accuracy: 0.9861\n",
      "Epoch 134/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.9861\n",
      "Epoch 135/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9861\n",
      "Epoch 136/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9861\n",
      "Epoch 137/200\n",
      "29/29 [==============================] - 0s 877us/step - loss: 0.0973 - accuracy: 0.9861\n",
      "Epoch 138/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9861\n",
      "Epoch 139/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9861\n",
      "Epoch 140/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.0980 - accuracy: 0.9861\n",
      "Epoch 141/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0974 - accuracy: 0.9861\n",
      "Epoch 142/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.0944 - accuracy: 0.9861\n",
      "Epoch 143/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0932 - accuracy: 0.9861\n",
      "Epoch 144/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9861\n",
      "Epoch 145/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9861\n",
      "Epoch 146/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9861\n",
      "Epoch 147/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9861\n",
      "Epoch 148/200\n",
      "29/29 [==============================] - 0s 860us/step - loss: 0.0905 - accuracy: 0.9861\n",
      "Epoch 149/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.9861\n",
      "Epoch 150/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9861\n",
      "Epoch 151/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9861\n",
      "Epoch 152/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.9861\n",
      "Epoch 153/200\n",
      "29/29 [==============================] - 0s 825us/step - loss: 0.0876 - accuracy: 0.9861\n",
      "Epoch 154/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9861\n",
      "Epoch 155/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.9861\n",
      "Epoch 156/200\n",
      "29/29 [==============================] - 0s 792us/step - loss: 0.0861 - accuracy: 0.9861\n",
      "Epoch 157/200\n",
      "29/29 [==============================] - 0s 929us/step - loss: 0.0857 - accuracy: 0.9861\n",
      "Epoch 158/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0851 - accuracy: 0.9861\n",
      "Epoch 159/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.9861\n",
      "Epoch 160/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0847 - accuracy: 0.9861\n",
      "Epoch 161/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9861\n",
      "Epoch 162/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0837 - accuracy: 0.9861\n",
      "Epoch 163/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0830 - accuracy: 0.9861\n",
      "Epoch 164/200\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.0825 - accuracy: 0.9861\n",
      "Epoch 165/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9861\n",
      "Epoch 166/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9861\n",
      "Epoch 167/200\n",
      "29/29 [==============================] - 0s 953us/step - loss: 0.0816 - accuracy: 0.9861\n",
      "Epoch 168/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.9861\n",
      "Epoch 169/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9861\n",
      "Epoch 170/200\n",
      "29/29 [==============================] - 0s 992us/step - loss: 0.0808 - accuracy: 0.9861\n",
      "Epoch 171/200\n",
      "29/29 [==============================] - 0s 968us/step - loss: 0.0807 - accuracy: 0.9861\n",
      "Epoch 172/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.9861\n",
      "Epoch 173/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.9861\n",
      "Epoch 174/200\n",
      "29/29 [==============================] - 0s 949us/step - loss: 0.0794 - accuracy: 0.9861\n",
      "Epoch 175/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.9861\n",
      "Epoch 176/200\n",
      "29/29 [==============================] - 0s 950us/step - loss: 0.0786 - accuracy: 0.9861\n",
      "Epoch 177/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0786 - accuracy: 0.9861\n",
      "Epoch 178/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.9861\n",
      "Epoch 179/200\n",
      "29/29 [==============================] - 0s 876us/step - loss: 0.0822 - accuracy: 0.9861\n",
      "Epoch 180/200\n",
      "29/29 [==============================] - 0s 885us/step - loss: 0.0790 - accuracy: 0.9861\n",
      "Epoch 181/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.9861\n",
      "Epoch 182/200\n",
      "29/29 [==============================] - 0s 715us/step - loss: 0.0804 - accuracy: 0.9861\n",
      "Epoch 183/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.9861\n",
      "Epoch 184/200\n",
      "29/29 [==============================] - 0s 735us/step - loss: 0.0810 - accuracy: 0.9861\n",
      "Epoch 185/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9861\n",
      "Epoch 186/200\n",
      "29/29 [==============================] - 0s 624us/step - loss: 0.1044 - accuracy: 0.9722\n",
      "Epoch 187/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2065 - accuracy: 0.9236\n",
      "Epoch 188/200\n",
      "29/29 [==============================] - 0s 981us/step - loss: 0.1145 - accuracy: 0.9722\n",
      "Epoch 189/200\n",
      "29/29 [==============================] - 0s 968us/step - loss: 0.0887 - accuracy: 0.9792\n",
      "Epoch 190/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0772 - accuracy: 0.9861\n",
      "Epoch 191/200\n",
      "29/29 [==============================] - 0s 864us/step - loss: 0.0758 - accuracy: 0.9861\n",
      "Epoch 192/200\n",
      "29/29 [==============================] - 0s 717us/step - loss: 0.0748 - accuracy: 0.9861\n",
      "Epoch 193/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9861\n",
      "Epoch 194/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.9861\n",
      "Epoch 195/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0740 - accuracy: 0.9861\n",
      "Epoch 196/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.9861\n",
      "Epoch 197/200\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0736 - accuracy: 0.9861\n",
      "Epoch 198/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0736 - accuracy: 0.9861\n",
      "Epoch 199/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9861\n",
      "Epoch 200/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.9861\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 학습 셋과 테스트 셋을 분리해 사용할 수 있게끔 import\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/SoongMoo/soldesk20231218/main/data/sonar3.csv')\n",
    "x = df.iloc[:,:60]\n",
    "y = df.iloc[:,60]\n",
    "\n",
    "# 학습 셋과 테스트 셋을 분리\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,shuffle=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(24,input_dim=60,activation='relu'))\n",
    "model.add(Dense(12,activation='relu'))\n",
    "model.add(Dense(6,activation='relu'))\n",
    "model.add(Dense(3,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "his = model.fit(x_train,y_train,epochs=200,batch_size=5)\n",
    "# 모델을 테스트셋에 적용해 정확도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "788cb160-cef8-46f4-aeee-73d69c613087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5143 - accuracy: 0.9206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.920634925365448"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_test,y_test)\n",
    "score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "505c094d-d5f6-4b36-95ee-07273864d6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 81ms/step\n",
      "[[0.8785678]]\n",
      "광물!\n"
     ]
    }
   ],
   "source": [
    "test_set = [0.02,0.0371,0.0428,0.0207,0.0954,0.0986,0.1539,0.1601,0.3109,0.2111,0.1609,0.1582,0.2238,0.0645,0.066,0.2273,0.31,0.2999,0.5078,0.4797,0.5783,0.5071,0.4328,0.555,0.6711,0.6415,0.7104,0.808,0.6791,0.3857,0.1307,0.2604,0.5121,0.7547,0.8537,0.8507,0.6692,0.6097,0.4943,0.2744,0.051,0.2834,0.2825,0.4256,0.2641,0.1386,0.1051,0.1343,0.0383,0.0324,0.0232,0.0027,0.0065,0.0159,0.0072,0.0167,0.018,0.0084,0.009,0.0032]\n",
    "predi = model.predict([test_set])\n",
    "print(predi)\n",
    "if predi >= 0.5:\n",
    "    print('광물!')\n",
    "else:\n",
    "    print('비 광물!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5573a741-aeb2-4ba3-b591-9dc1408a3a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./data/model/my_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e8162062-fd08-412d-856b-c8e02f5e9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a9e4d922-3de1-416b-8db8-dd35c3216893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# 모델 불러오기 위해 필요함!\n",
    "model = load_model('./data/model/my_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d96ba065-4a8c-410c-853e-1a5bb12d422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0278 - accuracy: 0.8333\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4254 - accuracy: 0.8333\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7296 - accuracy: 0.7805\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2745 - accuracy: 0.9024\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.9451 - accuracy: 0.8780\n",
      "정확도 : [0.8333333134651184, 0.8333333134651184, 0.7804877758026123, 0.9024389982223511, 0.8780487775802612]\n",
      "정확도 평균 : 0.8455284357070922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold # KFold를 import해야 겹 교차 검증 사용 가능.\n",
    "# k(정수)겹 교차 검증\n",
    "# 왜 해야 하냐? 베이스 데이터가 너무 작다면 과적합 오류가 발생할 확률이 높다. = 정확도가 떨어진다는 소리\n",
    "# 데이터를 여러개로 나누어서 교차검증하여 정확도를 올린다.\n",
    "k = 5\n",
    "# 분할하기 전에 샘플이 치우치지 않도록 섞어준다.\n",
    "kfold = KFold(n_splits = k,shuffle = True)\n",
    "\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24,input_dim=60,activation='relu'))\n",
    "    model.add(Dense(12,activation='relu'))\n",
    "    model.add(Dense(6,activation='relu'))\n",
    "    model.add(Dense(3,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "acc_score = []\n",
    "# k 교차 검증을 이용한 학습\n",
    "for train_index , test_index in kfold.split(x):\n",
    "    X_train,X_test = x.iloc[train_index,:],x.iloc[test_index,:] # 속성\n",
    "    Y_train,Y_test = y.iloc[train_index],y.iloc[test_index] # 클래스\n",
    "    model = model_fn()\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    history = model.fit(X_train,Y_train,epochs=200,batch_size=10,verbose=0)\n",
    "    # 정확도 구하기\n",
    "    accuracy = model.evaluate(X_test,Y_test)[1]\n",
    "    acc_score.append(accuracy)\n",
    "\n",
    "avg_acc_score = sum(acc_score) / k\n",
    "print(f'정확도 : {acc_score}')\n",
    "print(f'정확도 평균 : {avg_acc_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8ba53a31-ef0c-492c-b0ae-5d9678632864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "[[0.96099585]]\n",
      "광물!\n"
     ]
    }
   ],
   "source": [
    "test_set = [0.003,0.0371,0.0428,0.0207,0.0954,0.0986,0.1539,0.1601,0.3109,0.2111,0.1609,0.1582,0.2238,0.0645,0.066,0.2273,0.31,0.2999,0.5078,0.4797,0.5783,0.5071,0.4328,0.555,0.6711,0.6415,0.7104,0.808,0.6791,0.3857,0.1307,0.2604,0.5121,0.7547,0.8537,0.8507,0.6692,0.6097,0.4943,0.2744,0.051,0.2834,0.2825,0.4256,0.2641,0.1386,0.1051,0.1343,0.0383,0.0324,0.0232,0.0027,0.0065,0.0159,0.0072,0.0167,0.018,0.0084,0.009,0.0032]\n",
    "predi = model.predict([test_set])\n",
    "print(predi)\n",
    "if predi >= 0.5:\n",
    "    print('광물!')\n",
    "else:\n",
    "    print('비 광물!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fa32863b-2722-42eb-89e7-2aad969c5923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7.4</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0</th>\n",
       "      <th>1.9</th>\n",
       "      <th>0.076</th>\n",
       "      <th>11</th>\n",
       "      <th>34</th>\n",
       "      <th>0.9978</th>\n",
       "      <th>3.51</th>\n",
       "      <th>0.56</th>\n",
       "      <th>9.4</th>\n",
       "      <th>5</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6491</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6496 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       7.4   0.7     0  1.9  0.076    11     34   0.9978  3.51  0.56   9.4  5  \\\n",
       "0      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8  5   \n",
       "1      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8  5   \n",
       "2     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8  6   \n",
       "3      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4  5   \n",
       "4      7.4  0.66  0.00  1.8  0.075  13.0   40.0  0.99780  3.51  0.56   9.4  5   \n",
       "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ... ..   \n",
       "6491   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2  6   \n",
       "6492   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6  5   \n",
       "6493   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4  6   \n",
       "6494   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8  7   \n",
       "6495   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8  6   \n",
       "\n",
       "      1  \n",
       "0     1  \n",
       "1     1  \n",
       "2     1  \n",
       "3     1  \n",
       "4     1  \n",
       "...  ..  \n",
       "6491  0  \n",
       "6492  0  \n",
       "6493  0  \n",
       "6494  0  \n",
       "6495  0  \n",
       "\n",
       "[6496 rows x 13 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/SoongMoo/soldesk20231218/main/data/wine.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4b7acc25-b26f-4d4b-8d60-ab898b6579a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 27ms/step - loss: 0.9646 - accuracy: 0.2343 - val_loss: 0.7371 - val_accuracy: 0.3102\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6963 - accuracy: 0.6502 - val_loss: 0.6818 - val_accuracy: 0.8037\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6743 - accuracy: 0.8016 - val_loss: 0.6636 - val_accuracy: 0.8260\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6505 - accuracy: 0.8309 - val_loss: 0.6300 - val_accuracy: 0.8699\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6019 - accuracy: 0.8845 - val_loss: 0.5652 - val_accuracy: 0.9022\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5276 - accuracy: 0.9076 - val_loss: 0.4794 - val_accuracy: 0.9199\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4351 - accuracy: 0.9228 - val_loss: 0.3826 - val_accuracy: 0.9230\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3446 - accuracy: 0.9261 - val_loss: 0.3052 - val_accuracy: 0.9246\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2792 - accuracy: 0.9222 - val_loss: 0.2574 - val_accuracy: 0.9230\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2399 - accuracy: 0.9248 - val_loss: 0.2352 - val_accuracy: 0.9246\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2205 - accuracy: 0.9243 - val_loss: 0.2257 - val_accuracy: 0.9246\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2104 - accuracy: 0.9271 - val_loss: 0.2220 - val_accuracy: 0.9269\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2051 - accuracy: 0.9279 - val_loss: 0.2204 - val_accuracy: 0.9261\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2011 - accuracy: 0.9307 - val_loss: 0.2198 - val_accuracy: 0.9253\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1991 - accuracy: 0.9323 - val_loss: 0.2175 - val_accuracy: 0.9276\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1969 - accuracy: 0.9335 - val_loss: 0.2159 - val_accuracy: 0.9284\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1964 - accuracy: 0.9328 - val_loss: 0.2144 - val_accuracy: 0.9284\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1936 - accuracy: 0.9343 - val_loss: 0.2134 - val_accuracy: 0.9292\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1923 - accuracy: 0.9341 - val_loss: 0.2122 - val_accuracy: 0.9307\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1912 - accuracy: 0.9346 - val_loss: 0.2112 - val_accuracy: 0.9299\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1903 - accuracy: 0.9351 - val_loss: 0.2105 - val_accuracy: 0.9323\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1895 - accuracy: 0.9346 - val_loss: 0.2091 - val_accuracy: 0.9307\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1886 - accuracy: 0.9346 - val_loss: 0.2079 - val_accuracy: 0.9307\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1870 - accuracy: 0.9361 - val_loss: 0.2071 - val_accuracy: 0.9307\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1868 - accuracy: 0.9351 - val_loss: 0.2066 - val_accuracy: 0.9307\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1859 - accuracy: 0.9358 - val_loss: 0.2055 - val_accuracy: 0.9307\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1840 - accuracy: 0.9369 - val_loss: 0.2041 - val_accuracy: 0.9307\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1829 - accuracy: 0.9371 - val_loss: 0.2033 - val_accuracy: 0.9315\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1817 - accuracy: 0.9371 - val_loss: 0.2022 - val_accuracy: 0.9315\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1808 - accuracy: 0.9382 - val_loss: 0.2013 - val_accuracy: 0.9315\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1791 - accuracy: 0.9384 - val_loss: 0.2000 - val_accuracy: 0.9307\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1786 - accuracy: 0.9382 - val_loss: 0.1980 - val_accuracy: 0.9315\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1779 - accuracy: 0.9366 - val_loss: 0.1974 - val_accuracy: 0.9315\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1764 - accuracy: 0.9379 - val_loss: 0.1954 - val_accuracy: 0.9307\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1723 - accuracy: 0.9407 - val_loss: 0.1909 - val_accuracy: 0.9307\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1716 - accuracy: 0.9407 - val_loss: 0.1994 - val_accuracy: 0.9361\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1701 - accuracy: 0.9394 - val_loss: 0.1895 - val_accuracy: 0.9299\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1676 - accuracy: 0.9384 - val_loss: 0.1917 - val_accuracy: 0.9353\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1672 - accuracy: 0.9397 - val_loss: 0.1828 - val_accuracy: 0.9323\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1633 - accuracy: 0.9407 - val_loss: 0.1818 - val_accuracy: 0.9361\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1628 - accuracy: 0.9407 - val_loss: 0.1828 - val_accuracy: 0.9369\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1599 - accuracy: 0.9433 - val_loss: 0.1782 - val_accuracy: 0.9338\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1581 - accuracy: 0.9425 - val_loss: 0.1782 - val_accuracy: 0.9384\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1570 - accuracy: 0.9435 - val_loss: 0.1760 - val_accuracy: 0.9376\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1559 - accuracy: 0.9428 - val_loss: 0.1717 - val_accuracy: 0.9353\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1538 - accuracy: 0.9425 - val_loss: 0.1697 - val_accuracy: 0.9376\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1493 - accuracy: 0.9446 - val_loss: 0.1763 - val_accuracy: 0.9392\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1517 - accuracy: 0.9446 - val_loss: 0.1674 - val_accuracy: 0.9376\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1490 - accuracy: 0.9448 - val_loss: 0.1644 - val_accuracy: 0.9369\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1451 - accuracy: 0.9456 - val_loss: 0.1626 - val_accuracy: 0.9400\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1421 - accuracy: 0.9469 - val_loss: 0.1589 - val_accuracy: 0.9400\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1398 - accuracy: 0.9471 - val_loss: 0.1574 - val_accuracy: 0.9400\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1399 - accuracy: 0.9474 - val_loss: 0.1699 - val_accuracy: 0.9400\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1399 - accuracy: 0.9474 - val_loss: 0.1551 - val_accuracy: 0.9392\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1356 - accuracy: 0.9469 - val_loss: 0.1504 - val_accuracy: 0.9407\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1332 - accuracy: 0.9474 - val_loss: 0.1496 - val_accuracy: 0.9407\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1350 - accuracy: 0.9484 - val_loss: 0.1502 - val_accuracy: 0.9453\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1325 - accuracy: 0.9492 - val_loss: 0.1466 - val_accuracy: 0.9453\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1279 - accuracy: 0.9510 - val_loss: 0.1430 - val_accuracy: 0.9423\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1253 - accuracy: 0.9497 - val_loss: 0.1412 - val_accuracy: 0.9423\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1223 - accuracy: 0.9520 - val_loss: 0.1398 - val_accuracy: 0.9423\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1203 - accuracy: 0.9520 - val_loss: 0.1404 - val_accuracy: 0.9430\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1182 - accuracy: 0.9525 - val_loss: 0.1353 - val_accuracy: 0.9430\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1162 - accuracy: 0.9541 - val_loss: 0.1324 - val_accuracy: 0.9477\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1148 - accuracy: 0.9554 - val_loss: 0.1332 - val_accuracy: 0.9453\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1138 - accuracy: 0.9543 - val_loss: 0.1332 - val_accuracy: 0.9438\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1115 - accuracy: 0.9546 - val_loss: 0.1368 - val_accuracy: 0.9438\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1118 - accuracy: 0.9574 - val_loss: 0.1306 - val_accuracy: 0.9446\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1087 - accuracy: 0.9571 - val_loss: 0.1257 - val_accuracy: 0.9523\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1162 - accuracy: 0.9574 - val_loss: 0.1235 - val_accuracy: 0.9530\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1162 - accuracy: 0.9556 - val_loss: 0.1225 - val_accuracy: 0.9538\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1075 - accuracy: 0.9577 - val_loss: 0.1215 - val_accuracy: 0.9546\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1056 - accuracy: 0.9582 - val_loss: 0.1199 - val_accuracy: 0.9569\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1010 - accuracy: 0.9600 - val_loss: 0.1193 - val_accuracy: 0.9546\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0998 - accuracy: 0.9607 - val_loss: 0.1179 - val_accuracy: 0.9569\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0988 - accuracy: 0.9633 - val_loss: 0.1167 - val_accuracy: 0.9577\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0984 - accuracy: 0.9620 - val_loss: 0.1154 - val_accuracy: 0.9577\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0978 - accuracy: 0.9651 - val_loss: 0.1143 - val_accuracy: 0.9600\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0959 - accuracy: 0.9646 - val_loss: 0.1157 - val_accuracy: 0.9584\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0973 - accuracy: 0.9641 - val_loss: 0.1135 - val_accuracy: 0.9607\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0979 - accuracy: 0.9630 - val_loss: 0.1113 - val_accuracy: 0.9600\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0943 - accuracy: 0.9646 - val_loss: 0.1106 - val_accuracy: 0.9600\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0965 - accuracy: 0.9654 - val_loss: 0.1138 - val_accuracy: 0.9615\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0935 - accuracy: 0.9648 - val_loss: 0.1116 - val_accuracy: 0.9623\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0938 - accuracy: 0.9654 - val_loss: 0.1089 - val_accuracy: 0.9638\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0893 - accuracy: 0.9677 - val_loss: 0.1089 - val_accuracy: 0.9630\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0917 - accuracy: 0.9677 - val_loss: 0.1069 - val_accuracy: 0.9630\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0949 - accuracy: 0.9654 - val_loss: 0.1217 - val_accuracy: 0.9600\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0925 - accuracy: 0.9674 - val_loss: 0.1062 - val_accuracy: 0.9630\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0904 - accuracy: 0.9656 - val_loss: 0.1038 - val_accuracy: 0.9638\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0908 - accuracy: 0.9661 - val_loss: 0.1019 - val_accuracy: 0.9615\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0874 - accuracy: 0.9705 - val_loss: 0.1076 - val_accuracy: 0.9615\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0853 - accuracy: 0.9710 - val_loss: 0.1036 - val_accuracy: 0.9646\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0830 - accuracy: 0.9715 - val_loss: 0.1025 - val_accuracy: 0.9661\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0815 - accuracy: 0.9725 - val_loss: 0.1006 - val_accuracy: 0.9654\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0831 - accuracy: 0.9725 - val_loss: 0.1055 - val_accuracy: 0.9623\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0831 - accuracy: 0.9738 - val_loss: 0.1067 - val_accuracy: 0.9615\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0834 - accuracy: 0.9720 - val_loss: 0.1000 - val_accuracy: 0.9677\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0804 - accuracy: 0.9700 - val_loss: 0.0972 - val_accuracy: 0.9654\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0796 - accuracy: 0.9728 - val_loss: 0.0978 - val_accuracy: 0.9669\n"
     ]
    }
   ],
   "source": [
    "x = df.iloc[:,0:0+12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,shuffle=True)\n",
    "\n",
    "# 모델 구조 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(32,input_dim=12,activation='relu'))\n",
    "model.add(Dense(16,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(4,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# epochs=100 은 순전파 100번 역전파 100번 의미               , 검증 셋 / 25%로 줬음!\n",
    "#   학습 셋 60 : 검증 셋 20 : 테스트 셋 20 퍼센테이지 정도로 분할됨.\n",
    "his = model.fit(x_train,y_train,epochs=100,batch_size=500,validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cd22f4a0-39ef-43d3-a673-6ef59cbba75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9753845930099487"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_test,y_test)\n",
    "score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5c4a43e4-6ea9-4d47-8f3b-ead81e2403dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8/8 [==============================] - 1s 43ms/step - loss: 3.6909 - accuracy: 0.7511 - val_loss: 1.8851 - val_accuracy: 0.7821\n",
      "Epoch 2/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.1302 - accuracy: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sdedu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 17ms/step - loss: 1.2013 - accuracy: 0.8101 - val_loss: 0.2928 - val_accuracy: 0.8884\n",
      "Epoch 3/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4662 - accuracy: 0.7847 - val_loss: 0.2644 - val_accuracy: 0.9107\n",
      "Epoch 4/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.3614 - accuracy: 0.8958 - val_loss: 0.2429 - val_accuracy: 0.9222\n",
      "Epoch 5/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.2338 - accuracy: 0.9289 - val_loss: 0.2251 - val_accuracy: 0.9369\n",
      "Epoch 6/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.2236 - accuracy: 0.9317 - val_loss: 0.1706 - val_accuracy: 0.9446\n",
      "Epoch 7/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.2045 - accuracy: 0.9369 - val_loss: 0.1635 - val_accuracy: 0.9438\n",
      "Epoch 8/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1911 - accuracy: 0.9412 - val_loss: 0.1663 - val_accuracy: 0.9461\n",
      "Epoch 9/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.1863 - accuracy: 0.9435 - val_loss: 0.1564 - val_accuracy: 0.9492\n",
      "Epoch 10/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1828 - accuracy: 0.9428 - val_loss: 0.1551 - val_accuracy: 0.9477\n",
      "Epoch 11/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1786 - accuracy: 0.9451 - val_loss: 0.1534 - val_accuracy: 0.9477\n",
      "Epoch 12/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1755 - accuracy: 0.9435 - val_loss: 0.1512 - val_accuracy: 0.9484\n",
      "Epoch 13/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1729 - accuracy: 0.9433 - val_loss: 0.1484 - val_accuracy: 0.9477\n",
      "Epoch 14/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1709 - accuracy: 0.9443 - val_loss: 0.1470 - val_accuracy: 0.9484\n",
      "Epoch 15/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1689 - accuracy: 0.9430 - val_loss: 0.1459 - val_accuracy: 0.9484\n",
      "Epoch 16/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1667 - accuracy: 0.9451 - val_loss: 0.1442 - val_accuracy: 0.9484\n",
      "Epoch 17/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1651 - accuracy: 0.9443 - val_loss: 0.1445 - val_accuracy: 0.9477\n",
      "Epoch 18/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1633 - accuracy: 0.9443 - val_loss: 0.1423 - val_accuracy: 0.9484\n",
      "Epoch 19/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1622 - accuracy: 0.9446 - val_loss: 0.1422 - val_accuracy: 0.9500\n",
      "Epoch 20/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1607 - accuracy: 0.9443 - val_loss: 0.1407 - val_accuracy: 0.9492\n",
      "Epoch 21/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1588 - accuracy: 0.9448 - val_loss: 0.1399 - val_accuracy: 0.9492\n",
      "Epoch 22/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1576 - accuracy: 0.9441 - val_loss: 0.1392 - val_accuracy: 0.9492\n",
      "Epoch 23/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1565 - accuracy: 0.9443 - val_loss: 0.1391 - val_accuracy: 0.9515\n",
      "Epoch 24/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1553 - accuracy: 0.9446 - val_loss: 0.1368 - val_accuracy: 0.9523\n",
      "Epoch 25/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1545 - accuracy: 0.9448 - val_loss: 0.1369 - val_accuracy: 0.9515\n",
      "Epoch 26/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1537 - accuracy: 0.9441 - val_loss: 0.1372 - val_accuracy: 0.9523\n",
      "Epoch 27/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1528 - accuracy: 0.9438 - val_loss: 0.1349 - val_accuracy: 0.9530\n",
      "Epoch 28/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1513 - accuracy: 0.9438 - val_loss: 0.1413 - val_accuracy: 0.9469\n",
      "Epoch 29/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1534 - accuracy: 0.9443 - val_loss: 0.1330 - val_accuracy: 0.9530\n",
      "Epoch 30/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1493 - accuracy: 0.9443 - val_loss: 0.1321 - val_accuracy: 0.9530\n",
      "Epoch 31/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1487 - accuracy: 0.9453 - val_loss: 0.1335 - val_accuracy: 0.9530\n",
      "Epoch 32/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1469 - accuracy: 0.9448 - val_loss: 0.1315 - val_accuracy: 0.9538\n",
      "Epoch 33/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1455 - accuracy: 0.9453 - val_loss: 0.1304 - val_accuracy: 0.9554\n",
      "Epoch 34/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1446 - accuracy: 0.9461 - val_loss: 0.1306 - val_accuracy: 0.9554\n",
      "Epoch 35/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1440 - accuracy: 0.9456 - val_loss: 0.1308 - val_accuracy: 0.9554\n",
      "Epoch 36/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1437 - accuracy: 0.9471 - val_loss: 0.1292 - val_accuracy: 0.9546\n",
      "Epoch 37/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1419 - accuracy: 0.9469 - val_loss: 0.1274 - val_accuracy: 0.9569\n",
      "Epoch 38/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1410 - accuracy: 0.9477 - val_loss: 0.1257 - val_accuracy: 0.9569\n",
      "Epoch 39/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1396 - accuracy: 0.9477 - val_loss: 0.1276 - val_accuracy: 0.9554\n",
      "Epoch 40/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1393 - accuracy: 0.9482 - val_loss: 0.1252 - val_accuracy: 0.9554\n",
      "Epoch 41/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1395 - accuracy: 0.9469 - val_loss: 0.1283 - val_accuracy: 0.9546\n",
      "Epoch 42/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1385 - accuracy: 0.9482 - val_loss: 0.1248 - val_accuracy: 0.9577\n",
      "Epoch 43/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1366 - accuracy: 0.9494 - val_loss: 0.1224 - val_accuracy: 0.9577\n",
      "Epoch 44/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1357 - accuracy: 0.9484 - val_loss: 0.1219 - val_accuracy: 0.9584\n",
      "Epoch 45/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1356 - accuracy: 0.9492 - val_loss: 0.1220 - val_accuracy: 0.9584\n",
      "Epoch 46/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1353 - accuracy: 0.9492 - val_loss: 0.1223 - val_accuracy: 0.9577\n",
      "Epoch 47/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1338 - accuracy: 0.9492 - val_loss: 0.1204 - val_accuracy: 0.9584\n",
      "Epoch 48/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.1327 - accuracy: 0.9497 - val_loss: 0.1199 - val_accuracy: 0.9584\n",
      "Epoch 49/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1321 - accuracy: 0.9497 - val_loss: 0.1196 - val_accuracy: 0.9584\n",
      "Epoch 50/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1312 - accuracy: 0.9520 - val_loss: 0.1174 - val_accuracy: 0.9600\n",
      "Epoch 51/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1303 - accuracy: 0.9515 - val_loss: 0.1186 - val_accuracy: 0.9592\n",
      "Epoch 52/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1302 - accuracy: 0.9520 - val_loss: 0.1185 - val_accuracy: 0.9584\n",
      "Epoch 53/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1293 - accuracy: 0.9525 - val_loss: 0.1174 - val_accuracy: 0.9592\n",
      "Epoch 54/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1290 - accuracy: 0.9536 - val_loss: 0.1224 - val_accuracy: 0.9577\n",
      "Epoch 55/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1291 - accuracy: 0.9528 - val_loss: 0.1154 - val_accuracy: 0.9592\n",
      "Epoch 56/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1298 - accuracy: 0.9554 - val_loss: 0.1137 - val_accuracy: 0.9592\n",
      "Epoch 57/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1263 - accuracy: 0.9554 - val_loss: 0.1127 - val_accuracy: 0.9592\n",
      "Epoch 58/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1259 - accuracy: 0.9530 - val_loss: 0.1123 - val_accuracy: 0.9607\n",
      "Epoch 59/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1255 - accuracy: 0.9559 - val_loss: 0.1154 - val_accuracy: 0.9592\n",
      "Epoch 60/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1264 - accuracy: 0.9564 - val_loss: 0.1170 - val_accuracy: 0.9584\n",
      "Epoch 61/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1254 - accuracy: 0.9554 - val_loss: 0.1155 - val_accuracy: 0.9592\n",
      "Epoch 62/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1232 - accuracy: 0.9566 - val_loss: 0.1089 - val_accuracy: 0.9600\n",
      "Epoch 63/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1247 - accuracy: 0.9548 - val_loss: 0.1122 - val_accuracy: 0.9569\n",
      "Epoch 64/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1220 - accuracy: 0.9566 - val_loss: 0.1098 - val_accuracy: 0.9607\n",
      "Epoch 65/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1200 - accuracy: 0.9566 - val_loss: 0.1086 - val_accuracy: 0.9600\n",
      "Epoch 66/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1206 - accuracy: 0.9577 - val_loss: 0.1074 - val_accuracy: 0.9607\n",
      "Epoch 67/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1194 - accuracy: 0.9592 - val_loss: 0.1067 - val_accuracy: 0.9623\n",
      "Epoch 68/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1195 - accuracy: 0.9589 - val_loss: 0.1057 - val_accuracy: 0.9615\n",
      "Epoch 69/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1201 - accuracy: 0.9592 - val_loss: 0.1049 - val_accuracy: 0.9615\n",
      "Epoch 70/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1177 - accuracy: 0.9610 - val_loss: 0.1070 - val_accuracy: 0.9638\n",
      "Epoch 71/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1164 - accuracy: 0.9595 - val_loss: 0.1032 - val_accuracy: 0.9623\n",
      "Epoch 72/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1157 - accuracy: 0.9607 - val_loss: 0.1036 - val_accuracy: 0.9615\n",
      "Epoch 73/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1150 - accuracy: 0.9623 - val_loss: 0.1023 - val_accuracy: 0.9630\n",
      "Epoch 74/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1136 - accuracy: 0.9630 - val_loss: 0.1005 - val_accuracy: 0.9630\n",
      "Epoch 75/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1139 - accuracy: 0.9613 - val_loss: 0.1005 - val_accuracy: 0.9623\n",
      "Epoch 76/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1128 - accuracy: 0.9623 - val_loss: 0.1006 - val_accuracy: 0.9654\n",
      "Epoch 77/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1149 - accuracy: 0.9602 - val_loss: 0.0996 - val_accuracy: 0.9646\n",
      "Epoch 78/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1134 - accuracy: 0.9620 - val_loss: 0.0980 - val_accuracy: 0.9630\n",
      "Epoch 79/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1109 - accuracy: 0.9625 - val_loss: 0.0983 - val_accuracy: 0.9677\n",
      "Epoch 80/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1097 - accuracy: 0.9641 - val_loss: 0.0974 - val_accuracy: 0.9661\n",
      "Epoch 81/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1088 - accuracy: 0.9646 - val_loss: 0.0961 - val_accuracy: 0.9654\n",
      "Epoch 82/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1087 - accuracy: 0.9651 - val_loss: 0.0975 - val_accuracy: 0.9638\n",
      "Epoch 83/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1120 - accuracy: 0.9623 - val_loss: 0.0945 - val_accuracy: 0.9654\n",
      "Epoch 84/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1070 - accuracy: 0.9656 - val_loss: 0.0945 - val_accuracy: 0.9669\n",
      "Epoch 85/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1059 - accuracy: 0.9648 - val_loss: 0.0942 - val_accuracy: 0.9684\n",
      "Epoch 86/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1055 - accuracy: 0.9666 - val_loss: 0.0933 - val_accuracy: 0.9684\n",
      "Epoch 87/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1077 - accuracy: 0.9643 - val_loss: 0.0931 - val_accuracy: 0.9654\n",
      "Epoch 88/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1061 - accuracy: 0.9656 - val_loss: 0.0946 - val_accuracy: 0.9692\n",
      "Epoch 89/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1040 - accuracy: 0.9679 - val_loss: 0.0907 - val_accuracy: 0.9692\n",
      "Epoch 90/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1038 - accuracy: 0.9687 - val_loss: 0.0900 - val_accuracy: 0.9692\n",
      "Epoch 91/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1035 - accuracy: 0.9659 - val_loss: 0.0893 - val_accuracy: 0.9692\n",
      "Epoch 92/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1039 - accuracy: 0.9656 - val_loss: 0.0903 - val_accuracy: 0.9654\n",
      "Epoch 93/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1060 - accuracy: 0.9648 - val_loss: 0.0893 - val_accuracy: 0.9654\n",
      "Epoch 94/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1043 - accuracy: 0.9666 - val_loss: 0.0878 - val_accuracy: 0.9692\n",
      "Epoch 95/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1015 - accuracy: 0.9679 - val_loss: 0.0865 - val_accuracy: 0.9692\n",
      "Epoch 96/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1008 - accuracy: 0.9679 - val_loss: 0.0870 - val_accuracy: 0.9669\n",
      "Epoch 97/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0996 - accuracy: 0.9679 - val_loss: 0.0853 - val_accuracy: 0.9700\n",
      "Epoch 98/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0994 - accuracy: 0.9690 - val_loss: 0.0849 - val_accuracy: 0.9692\n",
      "Epoch 99/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0969 - accuracy: 0.9710 - val_loss: 0.0832 - val_accuracy: 0.9707\n",
      "Epoch 100/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0976 - accuracy: 0.9695 - val_loss: 0.0826 - val_accuracy: 0.9715\n",
      "Epoch 101/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0966 - accuracy: 0.9684 - val_loss: 0.0818 - val_accuracy: 0.9715\n",
      "Epoch 102/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0953 - accuracy: 0.9713 - val_loss: 0.0814 - val_accuracy: 0.9707\n",
      "Epoch 103/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0950 - accuracy: 0.9707 - val_loss: 0.0819 - val_accuracy: 0.9731\n",
      "Epoch 104/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0940 - accuracy: 0.9718 - val_loss: 0.0834 - val_accuracy: 0.9746\n",
      "Epoch 105/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0938 - accuracy: 0.9715 - val_loss: 0.0858 - val_accuracy: 0.9754\n",
      "Epoch 106/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0945 - accuracy: 0.9710 - val_loss: 0.0797 - val_accuracy: 0.9715\n",
      "Epoch 107/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0966 - accuracy: 0.9687 - val_loss: 0.0778 - val_accuracy: 0.9723\n",
      "Epoch 108/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0937 - accuracy: 0.9707 - val_loss: 0.0788 - val_accuracy: 0.9723\n",
      "Epoch 109/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0911 - accuracy: 0.9733 - val_loss: 0.0768 - val_accuracy: 0.9738\n",
      "Epoch 110/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0903 - accuracy: 0.9725 - val_loss: 0.0757 - val_accuracy: 0.9723\n",
      "Epoch 111/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0906 - accuracy: 0.9733 - val_loss: 0.0760 - val_accuracy: 0.9715\n",
      "Epoch 112/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0913 - accuracy: 0.9720 - val_loss: 0.0817 - val_accuracy: 0.9630\n",
      "Epoch 113/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0929 - accuracy: 0.9733 - val_loss: 0.0744 - val_accuracy: 0.9715\n",
      "Epoch 114/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0907 - accuracy: 0.9725 - val_loss: 0.0748 - val_accuracy: 0.9738\n",
      "Epoch 115/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0909 - accuracy: 0.9715 - val_loss: 0.0827 - val_accuracy: 0.9754\n",
      "Epoch 116/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0910 - accuracy: 0.9720 - val_loss: 0.0739 - val_accuracy: 0.9738\n",
      "Epoch 117/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0869 - accuracy: 0.9749 - val_loss: 0.0745 - val_accuracy: 0.9746\n",
      "Epoch 118/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0869 - accuracy: 0.9746 - val_loss: 0.0717 - val_accuracy: 0.9738\n",
      "Epoch 119/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0855 - accuracy: 0.9751 - val_loss: 0.0703 - val_accuracy: 0.9746\n",
      "Epoch 120/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0856 - accuracy: 0.9761 - val_loss: 0.0723 - val_accuracy: 0.9761\n",
      "Epoch 121/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0857 - accuracy: 0.9759 - val_loss: 0.0716 - val_accuracy: 0.9738\n",
      "Epoch 122/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0844 - accuracy: 0.9754 - val_loss: 0.0687 - val_accuracy: 0.9754\n",
      "Epoch 123/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0843 - accuracy: 0.9741 - val_loss: 0.0682 - val_accuracy: 0.9738\n",
      "Epoch 124/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0830 - accuracy: 0.9766 - val_loss: 0.0707 - val_accuracy: 0.9754\n",
      "Epoch 125/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0840 - accuracy: 0.9756 - val_loss: 0.0693 - val_accuracy: 0.9738\n",
      "Epoch 126/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0836 - accuracy: 0.9761 - val_loss: 0.0671 - val_accuracy: 0.9761\n",
      "Epoch 127/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0865 - accuracy: 0.9743 - val_loss: 0.0667 - val_accuracy: 0.9761\n",
      "Epoch 128/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0828 - accuracy: 0.9761 - val_loss: 0.0663 - val_accuracy: 0.9754\n",
      "Epoch 129/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0809 - accuracy: 0.9766 - val_loss: 0.0666 - val_accuracy: 0.9746\n",
      "Epoch 130/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0820 - accuracy: 0.9761 - val_loss: 0.0639 - val_accuracy: 0.9754\n",
      "Epoch 131/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0820 - accuracy: 0.9754 - val_loss: 0.0691 - val_accuracy: 0.9769\n",
      "Epoch 132/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0850 - accuracy: 0.9751 - val_loss: 0.0741 - val_accuracy: 0.9769\n",
      "Epoch 133/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0821 - accuracy: 0.9769 - val_loss: 0.0671 - val_accuracy: 0.9769\n",
      "Epoch 134/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0801 - accuracy: 0.9774 - val_loss: 0.0619 - val_accuracy: 0.9761\n",
      "Epoch 135/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0794 - accuracy: 0.9766 - val_loss: 0.0645 - val_accuracy: 0.9754\n",
      "Epoch 136/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0782 - accuracy: 0.9766 - val_loss: 0.0663 - val_accuracy: 0.9769\n",
      "Epoch 137/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0782 - accuracy: 0.9774 - val_loss: 0.0620 - val_accuracy: 0.9761\n",
      "Epoch 138/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0771 - accuracy: 0.9772 - val_loss: 0.0599 - val_accuracy: 0.9769\n",
      "Epoch 139/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0794 - accuracy: 0.9764 - val_loss: 0.0601 - val_accuracy: 0.9792\n",
      "Epoch 140/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0764 - accuracy: 0.9782 - val_loss: 0.0599 - val_accuracy: 0.9769\n",
      "Epoch 141/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0769 - accuracy: 0.9777 - val_loss: 0.0590 - val_accuracy: 0.9784\n",
      "Epoch 142/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0760 - accuracy: 0.9795 - val_loss: 0.0595 - val_accuracy: 0.9761\n",
      "Epoch 143/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0789 - accuracy: 0.9764 - val_loss: 0.0727 - val_accuracy: 0.9769\n",
      "Epoch 144/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0778 - accuracy: 0.9782 - val_loss: 0.0608 - val_accuracy: 0.9777\n",
      "Epoch 145/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0748 - accuracy: 0.9795 - val_loss: 0.0648 - val_accuracy: 0.9777\n",
      "Epoch 146/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0763 - accuracy: 0.9779 - val_loss: 0.0636 - val_accuracy: 0.9769\n",
      "Epoch 147/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0760 - accuracy: 0.9782 - val_loss: 0.0564 - val_accuracy: 0.9792\n",
      "Epoch 148/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0736 - accuracy: 0.9790 - val_loss: 0.0599 - val_accuracy: 0.9769\n",
      "Epoch 149/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0741 - accuracy: 0.9784 - val_loss: 0.0569 - val_accuracy: 0.9792\n",
      "Epoch 150/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0761 - accuracy: 0.9782 - val_loss: 0.0583 - val_accuracy: 0.9769\n",
      "Epoch 151/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0777 - accuracy: 0.9769 - val_loss: 0.0547 - val_accuracy: 0.9792\n",
      "Epoch 152/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0750 - accuracy: 0.9766 - val_loss: 0.0586 - val_accuracy: 0.9784\n",
      "Epoch 153/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0747 - accuracy: 0.9777 - val_loss: 0.0705 - val_accuracy: 0.9754\n",
      "Epoch 154/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0754 - accuracy: 0.9756 - val_loss: 0.0689 - val_accuracy: 0.9761\n",
      "Epoch 155/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0771 - accuracy: 0.9759 - val_loss: 0.0566 - val_accuracy: 0.9784\n",
      "Epoch 156/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0737 - accuracy: 0.9779 - val_loss: 0.0561 - val_accuracy: 0.9777\n",
      "Epoch 157/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0736 - accuracy: 0.9787 - val_loss: 0.0529 - val_accuracy: 0.9800\n",
      "Epoch 158/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0784 - accuracy: 0.9751 - val_loss: 0.0533 - val_accuracy: 0.9800\n",
      "Epoch 159/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0757 - accuracy: 0.9797 - val_loss: 0.0592 - val_accuracy: 0.9784\n",
      "Epoch 160/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0717 - accuracy: 0.9795 - val_loss: 0.0530 - val_accuracy: 0.9800\n",
      "Epoch 161/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0699 - accuracy: 0.9802 - val_loss: 0.0522 - val_accuracy: 0.9792\n",
      "Epoch 162/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0694 - accuracy: 0.9784 - val_loss: 0.0524 - val_accuracy: 0.9815\n",
      "Epoch 163/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0703 - accuracy: 0.9784 - val_loss: 0.0515 - val_accuracy: 0.9808\n",
      "Epoch 164/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0689 - accuracy: 0.9802 - val_loss: 0.0498 - val_accuracy: 0.9800\n",
      "Epoch 165/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0696 - accuracy: 0.9784 - val_loss: 0.0493 - val_accuracy: 0.9831\n",
      "Epoch 166/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0697 - accuracy: 0.9805 - val_loss: 0.0605 - val_accuracy: 0.9777\n",
      "Epoch 167/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0700 - accuracy: 0.9777 - val_loss: 0.0533 - val_accuracy: 0.9800\n",
      "Epoch 168/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0726 - accuracy: 0.9787 - val_loss: 0.0484 - val_accuracy: 0.9815\n",
      "Epoch 169/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0726 - accuracy: 0.9764 - val_loss: 0.0487 - val_accuracy: 0.9800\n",
      "Epoch 170/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0675 - accuracy: 0.9790 - val_loss: 0.0504 - val_accuracy: 0.9815\n",
      "Epoch 171/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0705 - accuracy: 0.9795 - val_loss: 0.0536 - val_accuracy: 0.9815\n",
      "Epoch 172/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0676 - accuracy: 0.9810 - val_loss: 0.0541 - val_accuracy: 0.9800\n",
      "Epoch 173/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0669 - accuracy: 0.9792 - val_loss: 0.0477 - val_accuracy: 0.9815\n",
      "Epoch 174/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0712 - accuracy: 0.9772 - val_loss: 0.0477 - val_accuracy: 0.9808\n",
      "Epoch 175/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0677 - accuracy: 0.9790 - val_loss: 0.0450 - val_accuracy: 0.9831\n",
      "Epoch 176/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0682 - accuracy: 0.9782 - val_loss: 0.0474 - val_accuracy: 0.9800\n",
      "Epoch 177/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0730 - accuracy: 0.9772 - val_loss: 0.0532 - val_accuracy: 0.9808\n",
      "Epoch 178/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0754 - accuracy: 0.9751 - val_loss: 0.0772 - val_accuracy: 0.9700\n",
      "Epoch 179/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0820 - accuracy: 0.9736 - val_loss: 0.0451 - val_accuracy: 0.9846\n",
      "Epoch 180/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0729 - accuracy: 0.9782 - val_loss: 0.0516 - val_accuracy: 0.9784\n",
      "Epoch 181/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0724 - accuracy: 0.9769 - val_loss: 0.0507 - val_accuracy: 0.9792\n",
      "Epoch 182/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0677 - accuracy: 0.9808 - val_loss: 0.0450 - val_accuracy: 0.9831\n",
      "Epoch 183/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0638 - accuracy: 0.9818 - val_loss: 0.0483 - val_accuracy: 0.9823\n",
      "Epoch 184/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0636 - accuracy: 0.9805 - val_loss: 0.0467 - val_accuracy: 0.9823\n",
      "Epoch 185/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0635 - accuracy: 0.9815 - val_loss: 0.0531 - val_accuracy: 0.9800\n",
      "Epoch 186/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0630 - accuracy: 0.9823 - val_loss: 0.0420 - val_accuracy: 0.9861\n",
      "Epoch 187/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0638 - accuracy: 0.9802 - val_loss: 0.0414 - val_accuracy: 0.9869\n",
      "Epoch 188/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0629 - accuracy: 0.9795 - val_loss: 0.0420 - val_accuracy: 0.9869\n",
      "Epoch 189/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0624 - accuracy: 0.9820 - val_loss: 0.0438 - val_accuracy: 0.9861\n",
      "Epoch 190/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0629 - accuracy: 0.9831 - val_loss: 0.0479 - val_accuracy: 0.9838\n",
      "Epoch 191/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0623 - accuracy: 0.9815 - val_loss: 0.0411 - val_accuracy: 0.9877\n",
      "Epoch 192/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0610 - accuracy: 0.9831 - val_loss: 0.0409 - val_accuracy: 0.9869\n",
      "Epoch 193/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0613 - accuracy: 0.9831 - val_loss: 0.0415 - val_accuracy: 0.9877\n",
      "Epoch 194/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0617 - accuracy: 0.9826 - val_loss: 0.0399 - val_accuracy: 0.9877\n",
      "Epoch 195/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0626 - accuracy: 0.9836 - val_loss: 0.0450 - val_accuracy: 0.9846\n",
      "Epoch 196/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0604 - accuracy: 0.9831 - val_loss: 0.0421 - val_accuracy: 0.9877\n",
      "Epoch 197/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0593 - accuracy: 0.9833 - val_loss: 0.0406 - val_accuracy: 0.9869\n",
      "Epoch 198/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.0410 - val_accuracy: 0.9869\n",
      "Epoch 199/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0595 - accuracy: 0.9836 - val_loss: 0.0391 - val_accuracy: 0.9869\n",
      "Epoch 200/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0595 - accuracy: 0.9841 - val_loss: 0.0418 - val_accuracy: 0.9877\n",
      "Epoch 201/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0598 - accuracy: 0.9823 - val_loss: 0.0366 - val_accuracy: 0.9892\n",
      "Epoch 202/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0609 - accuracy: 0.9818 - val_loss: 0.0464 - val_accuracy: 0.9854\n",
      "Epoch 203/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0618 - accuracy: 0.9813 - val_loss: 0.0388 - val_accuracy: 0.9869\n",
      "Epoch 204/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0616 - accuracy: 0.9833 - val_loss: 0.0388 - val_accuracy: 0.9854\n",
      "Epoch 205/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0638 - accuracy: 0.9808 - val_loss: 0.0444 - val_accuracy: 0.9823\n",
      "Epoch 206/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0680 - accuracy: 0.9792 - val_loss: 0.0409 - val_accuracy: 0.9877\n",
      "Epoch 207/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0589 - accuracy: 0.9841 - val_loss: 0.0381 - val_accuracy: 0.9877\n",
      "Epoch 208/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0571 - accuracy: 0.9833 - val_loss: 0.0395 - val_accuracy: 0.9861\n",
      "Epoch 209/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0383 - val_accuracy: 0.9869\n",
      "Epoch 210/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0582 - accuracy: 0.9838 - val_loss: 0.0350 - val_accuracy: 0.9900\n",
      "Epoch 211/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0598 - accuracy: 0.9826 - val_loss: 0.0358 - val_accuracy: 0.9900\n",
      "Epoch 212/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0630 - accuracy: 0.9792 - val_loss: 0.0372 - val_accuracy: 0.9861\n",
      "Epoch 213/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0602 - accuracy: 0.9833 - val_loss: 0.0345 - val_accuracy: 0.9892\n",
      "Epoch 214/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0591 - accuracy: 0.9810 - val_loss: 0.0413 - val_accuracy: 0.9861\n",
      "Epoch 215/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0596 - accuracy: 0.9810 - val_loss: 0.0364 - val_accuracy: 0.9869\n",
      "Epoch 216/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0589 - accuracy: 0.9818 - val_loss: 0.0374 - val_accuracy: 0.9854\n",
      "Epoch 217/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0584 - accuracy: 0.9823 - val_loss: 0.0362 - val_accuracy: 0.9877\n",
      "Epoch 218/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0562 - accuracy: 0.9833 - val_loss: 0.0350 - val_accuracy: 0.9885\n",
      "Epoch 219/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0621 - accuracy: 0.9818 - val_loss: 0.0459 - val_accuracy: 0.9861\n",
      "Epoch 220/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0577 - accuracy: 0.9826 - val_loss: 0.0335 - val_accuracy: 0.9915\n",
      "Epoch 221/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0551 - accuracy: 0.9851 - val_loss: 0.0352 - val_accuracy: 0.9877\n",
      "Epoch 222/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0560 - accuracy: 0.9826 - val_loss: 0.0342 - val_accuracy: 0.9877\n",
      "Epoch 223/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0568 - accuracy: 0.9826 - val_loss: 0.0340 - val_accuracy: 0.9900\n",
      "Epoch 224/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0558 - accuracy: 0.9818 - val_loss: 0.0410 - val_accuracy: 0.9885\n",
      "Epoch 225/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0549 - accuracy: 0.9843 - val_loss: 0.0328 - val_accuracy: 0.9900\n",
      "Epoch 226/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0549 - accuracy: 0.9843 - val_loss: 0.0374 - val_accuracy: 0.9885\n",
      "Epoch 227/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0543 - accuracy: 0.9836 - val_loss: 0.0347 - val_accuracy: 0.9908\n",
      "Epoch 228/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0542 - accuracy: 0.9838 - val_loss: 0.0355 - val_accuracy: 0.9892\n",
      "Epoch 229/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0567 - accuracy: 0.9823 - val_loss: 0.0330 - val_accuracy: 0.9885\n",
      "Epoch 230/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0562 - accuracy: 0.9820 - val_loss: 0.0337 - val_accuracy: 0.9885\n",
      "Epoch 231/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0527 - accuracy: 0.9841 - val_loss: 0.0345 - val_accuracy: 0.9885\n",
      "Epoch 232/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0529 - accuracy: 0.9843 - val_loss: 0.0387 - val_accuracy: 0.9877\n",
      "Epoch 233/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0555 - accuracy: 0.9838 - val_loss: 0.0338 - val_accuracy: 0.9885\n",
      "Epoch 234/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0543 - accuracy: 0.9826 - val_loss: 0.0326 - val_accuracy: 0.9892\n",
      "Epoch 235/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0538 - accuracy: 0.9838 - val_loss: 0.0368 - val_accuracy: 0.9885\n",
      "Epoch 236/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0535 - accuracy: 0.9841 - val_loss: 0.0332 - val_accuracy: 0.9908\n",
      "Epoch 237/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0577 - accuracy: 0.9823 - val_loss: 0.0364 - val_accuracy: 0.9838\n",
      "Epoch 238/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0599 - accuracy: 0.9815 - val_loss: 0.0332 - val_accuracy: 0.9892\n",
      "Epoch 239/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0517 - accuracy: 0.9851 - val_loss: 0.0343 - val_accuracy: 0.9908\n",
      "Epoch 240/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0541 - accuracy: 0.9836 - val_loss: 0.0343 - val_accuracy: 0.9885\n",
      "Epoch 241/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0536 - accuracy: 0.9849 - val_loss: 0.0320 - val_accuracy: 0.9892\n",
      "Epoch 242/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0520 - accuracy: 0.9833 - val_loss: 0.0408 - val_accuracy: 0.9900\n",
      "Epoch 243/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0596 - val_accuracy: 0.9823\n",
      "Epoch 244/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0569 - accuracy: 0.9836 - val_loss: 0.0466 - val_accuracy: 0.9869\n",
      "Epoch 245/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0557 - accuracy: 0.9851 - val_loss: 0.0317 - val_accuracy: 0.9892\n",
      "Epoch 246/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0525 - accuracy: 0.9833 - val_loss: 0.0319 - val_accuracy: 0.9892\n",
      "Epoch 247/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0529 - accuracy: 0.9838 - val_loss: 0.0364 - val_accuracy: 0.9885\n",
      "Epoch 248/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0519 - accuracy: 0.9849 - val_loss: 0.0394 - val_accuracy: 0.9892\n",
      "Epoch 249/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0502 - accuracy: 0.9836 - val_loss: 0.0492 - val_accuracy: 0.9877\n",
      "Epoch 250/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0546 - accuracy: 0.9846 - val_loss: 0.0431 - val_accuracy: 0.9877\n",
      "Epoch 251/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0510 - accuracy: 0.9846 - val_loss: 0.0379 - val_accuracy: 0.9908\n",
      "Epoch 252/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0503 - accuracy: 0.9846 - val_loss: 0.0391 - val_accuracy: 0.9908\n",
      "Epoch 253/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0519 - accuracy: 0.9856 - val_loss: 0.0372 - val_accuracy: 0.9915\n",
      "Epoch 254/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0506 - accuracy: 0.9849 - val_loss: 0.0394 - val_accuracy: 0.9908\n",
      "Epoch 255/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0542 - accuracy: 0.9826 - val_loss: 0.0426 - val_accuracy: 0.9900\n",
      "Epoch 256/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0500 - accuracy: 0.9861 - val_loss: 0.0360 - val_accuracy: 0.9908\n",
      "Epoch 257/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0523 - accuracy: 0.9851 - val_loss: 0.0353 - val_accuracy: 0.9915\n",
      "Epoch 258/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0529 - accuracy: 0.9846 - val_loss: 0.0320 - val_accuracy: 0.9885\n",
      "Epoch 259/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0500 - accuracy: 0.9851 - val_loss: 0.0301 - val_accuracy: 0.9892\n",
      "Epoch 260/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0505 - accuracy: 0.9843 - val_loss: 0.0321 - val_accuracy: 0.9923\n",
      "Epoch 261/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0492 - accuracy: 0.9861 - val_loss: 0.0331 - val_accuracy: 0.9900\n",
      "Epoch 262/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0498 - accuracy: 0.9861 - val_loss: 0.0345 - val_accuracy: 0.9861\n",
      "Epoch 263/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0512 - accuracy: 0.9854 - val_loss: 0.0299 - val_accuracy: 0.9900\n",
      "Epoch 264/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0508 - accuracy: 0.9859 - val_loss: 0.0300 - val_accuracy: 0.9900\n",
      "Epoch 265/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0541 - accuracy: 0.9849 - val_loss: 0.0442 - val_accuracy: 0.9900\n",
      "Epoch 266/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0585 - accuracy: 0.9820 - val_loss: 0.0315 - val_accuracy: 0.9892\n",
      "Epoch 267/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0594 - accuracy: 0.9818 - val_loss: 0.0306 - val_accuracy: 0.9908\n",
      "Epoch 268/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0500 - accuracy: 0.9856 - val_loss: 0.0314 - val_accuracy: 0.9892\n",
      "Epoch 269/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0519 - accuracy: 0.9856 - val_loss: 0.0306 - val_accuracy: 0.9885\n",
      "Epoch 270/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0492 - accuracy: 0.9846 - val_loss: 0.0336 - val_accuracy: 0.9908\n",
      "Epoch 271/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0480 - accuracy: 0.9851 - val_loss: 0.0623 - val_accuracy: 0.9823\n",
      "Epoch 272/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0591 - accuracy: 0.9833 - val_loss: 0.0304 - val_accuracy: 0.9900\n",
      "Epoch 273/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0502 - accuracy: 0.9854 - val_loss: 0.0302 - val_accuracy: 0.9900\n",
      "Epoch 274/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0484 - accuracy: 0.9867 - val_loss: 0.0319 - val_accuracy: 0.9908\n",
      "Epoch 275/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0482 - accuracy: 0.9859 - val_loss: 0.0302 - val_accuracy: 0.9900\n",
      "Epoch 276/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0483 - accuracy: 0.9869 - val_loss: 0.0304 - val_accuracy: 0.9885\n",
      "Epoch 277/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0480 - accuracy: 0.9861 - val_loss: 0.0301 - val_accuracy: 0.9900\n",
      "Epoch 278/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0474 - accuracy: 0.9854 - val_loss: 0.0298 - val_accuracy: 0.9892\n",
      "Epoch 279/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0519 - accuracy: 0.9849 - val_loss: 0.0450 - val_accuracy: 0.9892\n",
      "Epoch 280/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0479 - accuracy: 0.9861 - val_loss: 0.0448 - val_accuracy: 0.9892\n",
      "Epoch 281/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0512 - accuracy: 0.9861 - val_loss: 0.0344 - val_accuracy: 0.9915\n",
      "Epoch 282/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0507 - accuracy: 0.9869 - val_loss: 0.0310 - val_accuracy: 0.9885\n",
      "Epoch 283/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0520 - accuracy: 0.9851 - val_loss: 0.0330 - val_accuracy: 0.9869\n",
      "Epoch 284/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0519 - accuracy: 0.9859 - val_loss: 0.0338 - val_accuracy: 0.9908\n",
      "Epoch 285/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0479 - accuracy: 0.9874 - val_loss: 0.0315 - val_accuracy: 0.9885\n",
      "Epoch 286/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0476 - accuracy: 0.9861 - val_loss: 0.0311 - val_accuracy: 0.9915\n",
      "Epoch 287/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0478 - accuracy: 0.9874 - val_loss: 0.0343 - val_accuracy: 0.9923\n",
      "Epoch 288/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0473 - accuracy: 0.9869 - val_loss: 0.0333 - val_accuracy: 0.9923\n",
      "Epoch 289/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0477 - accuracy: 0.9874 - val_loss: 0.0316 - val_accuracy: 0.9915\n",
      "Epoch 290/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0488 - accuracy: 0.9867 - val_loss: 0.0347 - val_accuracy: 0.9931\n",
      "Epoch 291/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0499 - accuracy: 0.9877 - val_loss: 0.0301 - val_accuracy: 0.9908\n",
      "Epoch 292/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0456 - accuracy: 0.9879 - val_loss: 0.0313 - val_accuracy: 0.9892\n",
      "Epoch 293/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0486 - accuracy: 0.9859 - val_loss: 0.0299 - val_accuracy: 0.9900\n",
      "Epoch 294/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0490 - accuracy: 0.9856 - val_loss: 0.0350 - val_accuracy: 0.9923\n",
      "Epoch 295/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0464 - accuracy: 0.9877 - val_loss: 0.0312 - val_accuracy: 0.9915\n",
      "Epoch 296/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0467 - accuracy: 0.9872 - val_loss: 0.0341 - val_accuracy: 0.9908\n",
      "Epoch 297/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0460 - accuracy: 0.9869 - val_loss: 0.0313 - val_accuracy: 0.9915\n",
      "Epoch 298/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0460 - accuracy: 0.9877 - val_loss: 0.0305 - val_accuracy: 0.9923\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/SoongMoo/soldesk20231218/main/data/wine.csv') # header=None 주면 헤더는 빼고 가져올 수도 있음!\n",
    "\n",
    "x = df.iloc[:,0:0+12]\n",
    "y = df.iloc[:,12]\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,shuffle=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32,input_dim=12,activation='relu'))\n",
    "model.add(Dense(16,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(4,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# 학습이 언제 자동중단 될지 설정\n",
    "early_stopping_callback = EarlyStopping (monitor = 'val_loss' , patience = 20)\n",
    "\n",
    "# 최적화 모델이 저장될 폴더와 모델의 이름을 정하기\n",
    "modelpath = \"./data/model/bestmodel.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath,monitor = 'val_loss',verbose=0,save_best_only=True)\n",
    "\n",
    "# 모델 실행\n",
    "history = model.fit(x_train,y_train,epochs=2000,batch_size=500,validation_split=0.25,verbose=1,callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e3c93531-b01d-4952-b11b-58025b64f88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 1ms/step - loss: 0.0453 - accuracy: 0.9854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9853846430778503"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_test,y_test)\n",
    "score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3924dc2c-9bc3-4476-87a2-3790bb219e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
